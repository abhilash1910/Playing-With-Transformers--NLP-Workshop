{"cells":[{"metadata":{},"cell_type":"markdown","source":"# NLP Workshop- ML India\n\n\n\n## Understanding Transformers\n\n\nIn this session, we will be having an indepth overview of what Transformers are and how simply we can use them for any sentiment analysis or any language modelling task in general. Tre traditional methods of deep learning ,using LSTM variants for processing data in the form of sequential networks has underwent a severe modification and forms the current building blocks of all the SOTA transformer models that we see today. In this case, we will start will traditional neural networks to understand how they work and then try to build a simple transformer using those building blocks. Once that is completed, we will be leveraging state of the art open source libraries for our use case. Keeping the transformer architecture for reference,which we will be visiting again as we progress. \n\n\n<img src=\"https://miro.medium.com/max/500/1*do7YDFF2sads0p9BnjzrWA.png\">\n\n\n\n\n\nIn the first section we will be using simple neural networks for our use case. We will be understanding about the traditional LSTM and Recurrent Neural Networks.\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"# Basics of Neural Networks\n\nIn this scope, we will be looking into standard neural networks with very little modifications and minimalistic codebase. For simplicity we will be using Keras Framework:\n\n<img src=\"https://keras.io/img/logo.png\">"},{"metadata":{},"cell_type":"markdown","source":"# Building a Bare Minimal Neural Network \n\nHere, we will be building a stand-alonw neural network model just for classifying the labels to the respective questions. For this we will be using RNNs/LSTMs/GRU for our usecase. A classic LSTM based network is one of the most fundamental building blocks of all the robust architectures that we see today.For the first part we will be focussing on standard RNNs. Some resources for RNNs:\n\n- [NLP](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)\n- [RNN](https://www.kaggle.com/abhilash1910/nlp-workshop-ml-india)\n\n\n<img src=\"https://miro.medium.com/max/875/1*n-IgHZM5baBUjq0T7RYDBw.gif\">\n\n\n## Recurrent Neural Networks\n\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.Schematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far. Forward pass of Classical RNNs have the following formula :\n\n\n## Forward Pass Formula \n\n\n For the hidden gates: <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2017/12/06005300/eq2.png\">\n \n For the output gate: <img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2017/12/06005750/outeq.png\">\n \n \nGenerally for the output of the forward pass, we generally use a softmax activation on the output.\n\n\n## Backward Pass Equations and BPTT\n\n[Backpropagation Through Time](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/) is done in RNNs which allows flow of gradients through each hidden time step. The effective loss function for RNNs is : \n\n<img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++E_t%28y_t%2C+%5Chat%7By%7D_t%29+%26%3D+-+y_%7Bt%7D+%5Clog+%5Chat%7By%7D_%7Bt%7D+%5C%5C++E%28y%2C+%5Chat%7By%7D%29+%26%3D%5Csum%5Climits_%7Bt%7D+E_t%28y_t%2C%5Chat%7By%7D_t%29+%5C%5C++%26+%3D+-%5Csum%5Climits_%7Bt%7D+y_%7Bt%7D+%5Clog+%5Chat%7By%7D_%7Bt%7D++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\n\nour goal is to calculate the gradients of the error with respect to our parameters U, V and W and then learn good parameters using Stochastic Gradient Descent. Just like we sum up the errors, we also sum up the gradients at each time step for one training example: <img src=\"http://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+E%7D%7B%5Cpartial+W%7D+%3D+%5Csum%5Climits_%7Bt%7D+%5Cfrac%7B%5Cpartial+E_t%7D%7B%5Cpartial+W%7D&bg=ffffff&fg=000&s=1\">\n\nTo calculate these gradients we use the chain rule of differentiation. That’s the backpropagation algorithm when applied backwards starting from the error. For the rest of this post we’ll use E_3 as an example, just to have concrete numbers to work with.\n\n<img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+V%7D+%26%3D%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+V%7D%5C%5C++%26%3D%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+z_3%7D%5Cfrac%7B%5Cpartial+z_3%7D%7B%5Cpartial+V%7D%5C%5C++%26%3D%28%5Chat%7By%7D_3+-+y_3%29+%5Cotimes+s_3+%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\n\nEffectively the logic behind the chain rule is denoted by the following formula:\n<img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+W%7D+%26%3D+%5Csum%5Climits_%7Bk%3D0%7D%5E%7B3%7D+%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+s_3%7D%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_k%7D%5Cfrac%7B%5Cpartial+s_k%7D%7B%5Cpartial+W%7D%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\n\nBPTT can be understood clearly with this image\n\n\n<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTBrnLrCdE0ae14fv4Wc7ThY61Ikr6lzsyJSQ&usqp=CAU\">\n\n\n\n## Classical RNN image\n\n\nA classic RNN consists of the following image:\n\n\n<img src=\"https://miro.medium.com/max/627/1*go8PHsPNbbV6qRiwpUQ5BQ.png\">\n\n\nSome resources:\n    \n- [Blog](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n- [Video](https://youtu.be/-eBjweSRgFc)\n- [Blog](https://towardsdatascience.com/under-the-hood-of-neural-networks-part-2-recurrent-af091247ba78)\n- [Blog](https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/)\n- [Documentation](https://keras.io/api/layers/recurrent_layers/simple_rnn/)\n\n## Drawbacks of RNNs\n\n\nVanishing Gradients: The chain rule of differentiation of the weight vectors often lead to shrinkage in the change in the weights of the gradients for each iteration. This leads to  a slower convergence and many times it leads to an oscillation around local minimas. The chain rule formula: <img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+W%7D+%26%3D+%5Csum%5Climits_%7Bk%3D0%7D%5E%7B3%7D+%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+s_3%7D%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_k%7D%5Cfrac%7B%5Cpartial+s_k%7D%7B%5Cpartial+W%7D%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\nNote that <img src=\"http://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_k%7D+&bg=ffffff&fg=000&s=1\">  is a chain rule in itself! For example, <img src=\"http://s0.wp.com/latex.php?latex=%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_1%7D+%3D%5Cfrac%7B%5Cpartial+s_3%7D%7B%5Cpartial+s_2%7D%5Cfrac%7B%5Cpartial+s_2%7D%7B%5Cpartial+s_1%7D&bg=ffffff&fg=000&s=1\">. Also note that because we are taking the derivative of a vector function with respect to a vector, the result is a matrix (called the Jacobian matrix) whose elements are all the pointwise derivatives. We can rewrite the above gradient:\n\n<img src=\"http://s0.wp.com/latex.php?latex=%5Cbegin%7Baligned%7D++%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+W%7D+%26%3D+%5Csum%5Climits_%7Bk%3D0%7D%5E%7B3%7D+%5Cfrac%7B%5Cpartial+E_3%7D%7B%5Cpartial+%5Chat%7By%7D_3%7D%5Cfrac%7B%5Cpartial%5Chat%7By%7D_3%7D%7B%5Cpartial+s_3%7D++%5Cleft%28%5Cprod%5Climits_%7Bj%3Dk%2B1%7D%5E%7B3%7D++%5Cfrac%7B%5Cpartial+s_j%7D%7B%5Cpartial+s_%7Bj-1%7D%7D%5Cright%29++%5Cfrac%7B%5Cpartial+s_k%7D%7B%5Cpartial+W%7D%5C%5C++%5Cend%7Baligned%7D++&bg=ffffff&fg=000&s=0\">\n\nExploding Gradients: The chain rule (mainly due to tanh activation) often leads to overshooting of the gradient weights.This may lead to gradients which are really large at each iteration of the training process.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.200d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt\n/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\n/kaggle/input/google-quest-challenge/sample_submission.csv\n/kaggle/input/google-quest-challenge/train.csv\n/kaggle/input/google-quest-challenge/test.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df=pd.read_csv('../input/google-quest-challenge/train.csv')\ntrain_df.head()","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"   qa_id                                     question_title  \\\n0      0  What am I losing when using extension tubes in...   \n1      1  What is the distinction between a city and a s...   \n2      2  Maximum protusion length for through-hole comp...   \n3      3              Can an affidavit be used in Beit Din?   \n4      5       How do you make a binary image in Photoshop?   \n\n                                       question_body question_user_name  \\\n0  After playing around with macro photography on...               ysap   \n1  I am trying to understand what kinds of places...      russellpierce   \n2  I'm working on a PCB that has through-hole com...          Joe Baker   \n3  An affidavit, from what i understand, is basic...         Scimonster   \n4  I am trying to make a binary image. I want mor...            leigero   \n\n                                  question_user_page  \\\n0         https://photo.stackexchange.com/users/1024   \n1           https://rpg.stackexchange.com/users/8774   \n2  https://electronics.stackexchange.com/users/10157   \n3       https://judaism.stackexchange.com/users/5151   \n4  https://graphicdesign.stackexchange.com/users/...   \n\n                                              answer answer_user_name  \\\n0  I just got extension tubes, so here's the skin...           rfusca   \n1  It might be helpful to look into the definitio...     Erik Schmidt   \n2  Do you even need grooves?  We make several pro...      Dwayne Reid   \n3  Sending an \"affidavit\" it is a dispute between...    Y     e     z   \n4  Check out Image Trace in Adobe Illustrator. \\n...             q2ra   \n\n                                    answer_user_page  \\\n0         https://photo.stackexchange.com/users/1917   \n1           https://rpg.stackexchange.com/users/1871   \n2  https://electronics.stackexchange.com/users/64754   \n3       https://judaism.stackexchange.com/users/4794   \n4  https://graphicdesign.stackexchange.com/users/...   \n\n                                                 url   category  ...  \\\n0  http://photo.stackexchange.com/questions/9169/...  LIFE_ARTS  ...   \n1  http://rpg.stackexchange.com/questions/47820/w...    CULTURE  ...   \n2  http://electronics.stackexchange.com/questions...    SCIENCE  ...   \n3  http://judaism.stackexchange.com/questions/551...    CULTURE  ...   \n4  http://graphicdesign.stackexchange.com/questio...  LIFE_ARTS  ...   \n\n  question_well_written  answer_helpful  answer_level_of_information  \\\n0              1.000000        1.000000                     0.666667   \n1              0.888889        0.888889                     0.555556   \n2              0.777778        0.777778                     0.555556   \n3              0.888889        0.833333                     0.333333   \n4              1.000000        1.000000                     0.666667   \n\n   answer_plausible  answer_relevance  answer_satisfaction  \\\n0          1.000000          1.000000             0.800000   \n1          0.888889          0.888889             0.666667   \n2          1.000000          1.000000             0.666667   \n3          0.833333          1.000000             0.800000   \n4          1.000000          1.000000             0.800000   \n\n   answer_type_instructions  answer_type_procedure  \\\n0                       1.0               0.000000   \n1                       0.0               0.000000   \n2                       0.0               0.333333   \n3                       0.0               0.000000   \n4                       1.0               0.000000   \n\n   answer_type_reason_explanation  answer_well_written  \n0                        0.000000             1.000000  \n1                        0.666667             0.888889  \n2                        1.000000             0.888889  \n3                        1.000000             1.000000  \n4                        1.000000             1.000000  \n\n[5 rows x 41 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qa_id</th>\n      <th>question_title</th>\n      <th>question_body</th>\n      <th>question_user_name</th>\n      <th>question_user_page</th>\n      <th>answer</th>\n      <th>answer_user_name</th>\n      <th>answer_user_page</th>\n      <th>url</th>\n      <th>category</th>\n      <th>...</th>\n      <th>question_well_written</th>\n      <th>answer_helpful</th>\n      <th>answer_level_of_information</th>\n      <th>answer_plausible</th>\n      <th>answer_relevance</th>\n      <th>answer_satisfaction</th>\n      <th>answer_type_instructions</th>\n      <th>answer_type_procedure</th>\n      <th>answer_type_reason_explanation</th>\n      <th>answer_well_written</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>What am I losing when using extension tubes in...</td>\n      <td>After playing around with macro photography on...</td>\n      <td>ysap</td>\n      <td>https://photo.stackexchange.com/users/1024</td>\n      <td>I just got extension tubes, so here's the skin...</td>\n      <td>rfusca</td>\n      <td>https://photo.stackexchange.com/users/1917</td>\n      <td>http://photo.stackexchange.com/questions/9169/...</td>\n      <td>LIFE_ARTS</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>What is the distinction between a city and a s...</td>\n      <td>I am trying to understand what kinds of places...</td>\n      <td>russellpierce</td>\n      <td>https://rpg.stackexchange.com/users/8774</td>\n      <td>It might be helpful to look into the definitio...</td>\n      <td>Erik Schmidt</td>\n      <td>https://rpg.stackexchange.com/users/1871</td>\n      <td>http://rpg.stackexchange.com/questions/47820/w...</td>\n      <td>CULTURE</td>\n      <td>...</td>\n      <td>0.888889</td>\n      <td>0.888889</td>\n      <td>0.555556</td>\n      <td>0.888889</td>\n      <td>0.888889</td>\n      <td>0.666667</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.666667</td>\n      <td>0.888889</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Maximum protusion length for through-hole comp...</td>\n      <td>I'm working on a PCB that has through-hole com...</td>\n      <td>Joe Baker</td>\n      <td>https://electronics.stackexchange.com/users/10157</td>\n      <td>Do you even need grooves?  We make several pro...</td>\n      <td>Dwayne Reid</td>\n      <td>https://electronics.stackexchange.com/users/64754</td>\n      <td>http://electronics.stackexchange.com/questions...</td>\n      <td>SCIENCE</td>\n      <td>...</td>\n      <td>0.777778</td>\n      <td>0.777778</td>\n      <td>0.555556</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>1.000000</td>\n      <td>0.888889</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Can an affidavit be used in Beit Din?</td>\n      <td>An affidavit, from what i understand, is basic...</td>\n      <td>Scimonster</td>\n      <td>https://judaism.stackexchange.com/users/5151</td>\n      <td>Sending an \"affidavit\" it is a dispute between...</td>\n      <td>Y     e     z</td>\n      <td>https://judaism.stackexchange.com/users/4794</td>\n      <td>http://judaism.stackexchange.com/questions/551...</td>\n      <td>CULTURE</td>\n      <td>...</td>\n      <td>0.888889</td>\n      <td>0.833333</td>\n      <td>0.333333</td>\n      <td>0.833333</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>How do you make a binary image in Photoshop?</td>\n      <td>I am trying to make a binary image. I want mor...</td>\n      <td>leigero</td>\n      <td>https://graphicdesign.stackexchange.com/users/...</td>\n      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n      <td>q2ra</td>\n      <td>https://graphicdesign.stackexchange.com/users/...</td>\n      <td>http://graphicdesign.stackexchange.com/questio...</td>\n      <td>LIFE_ARTS</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 41 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Articulating the Problem statement\n\nIn the Google Quest QA challenge we will be using 'question_body' and 'category' and build a classifier (sentiment) and test the performance on this discriminative dataset. Along with this, we will be focussing later,on how to create appropriate answers from the questions using Transformers. For the initial aspects, we will be focussing on how to create simple models and encoder decoders."},{"metadata":{"trusted":true},"cell_type":"code","source":"import transformers\nfrom transformers import AutoTokenizer,AutoModelForQuestionAnswering\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D,SimpleRNN\nfrom keras.optimizers import Adam\nimport numpy as np  \nimport pandas as pd \nimport keras.backend as k\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,GRU\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nimport matplotlib.pyplot as plt","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This is used to label encode the labels for categorization\nfrom sklearn.preprocessing import LabelEncoder\nlabel_y= LabelEncoder()\nlabels=label_y.fit_transform(train_df['category'])\nlabels","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"array([1, 0, 2, ..., 4, 2, 1])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple RNN- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nz=Bidirectional(SimpleRNN(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_RNN.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":5,"outputs":[{"output_type":"stream","text":"Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 1000)]            0         \n_________________________________________________________________\nembedding (Embedding)        (None, 1000, 768)         3840000   \n_________________________________________________________________\nbidirectional (Bidirectional (None, 1000, 120)         99480     \n_________________________________________________________________\nglobal_max_pooling1d (Global (None, 120)               0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                1936      \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 85        \n=================================================================\nTotal params: 3,941,501\nTrainable params: 3,941,501\nNon-trainable params: 0\n_________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEpoch 1/10\n38/38 - 121s - loss: 1.4302 - accuracy: 0.4028 - val_loss: 1.2708 - val_accuracy: 0.4910\nEpoch 2/10\n38/38 - 109s - loss: 1.0469 - accuracy: 0.5731 - val_loss: 0.9236 - val_accuracy: 0.6530\nEpoch 3/10\n38/38 - 109s - loss: 0.6391 - accuracy: 0.8281 - val_loss: 0.6623 - val_accuracy: 0.7747\nEpoch 4/10\n38/38 - 109s - loss: 0.3170 - accuracy: 0.9576 - val_loss: 0.4420 - val_accuracy: 0.8561\nEpoch 5/10\n38/38 - 109s - loss: 0.1276 - accuracy: 0.9879 - val_loss: 0.4038 - val_accuracy: 0.8701\nEpoch 6/10\n38/38 - 109s - loss: 0.0517 - accuracy: 0.9984 - val_loss: 0.3518 - val_accuracy: 0.8816\nEpoch 7/10\n38/38 - 123s - loss: 0.0243 - accuracy: 0.9992 - val_loss: 0.3859 - val_accuracy: 0.8783\nEpoch 8/10\n38/38 - 109s - loss: 0.0141 - accuracy: 0.9998 - val_loss: 0.3571 - val_accuracy: 0.8857\nEpoch 9/10\n38/38 - 111s - loss: 0.0093 - accuracy: 0.9998 - val_loss: 0.3823 - val_accuracy: 0.8824\nEpoch 10/10\n38/38 - 110s - loss: 0.0069 - accuracy: 0.9998 - val_loss: 0.3816 - val_accuracy: 0.8808\n","name":"stdout"},{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7ff9c01c9f50>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture\n\nThe model architecture for the Bidirectional Simple RNN can be seen as below:\n\n<img src=\"https://i.imgur.com/QFsESSn.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Using Glove Embeddings, In this case, we will be using pretrained Glove 200dimension embeddings.\n#The importance of using pretrained embeddings is to allow more semantic references of the word/sentence vectors.\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\nEMBEDDING_FILE = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\nplt.plot(embedding_matrix[10])","execution_count":5,"outputs":[{"output_type":"stream","text":"Padded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  if (await self.run_code(code, result,  async_=asy)):\n","name":"stderr"},{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f1a31d4fd10>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using with pretrained Glove 200d embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=200\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding -with pretrained glove weights\n#3.Simple RNN- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(SimpleRNN(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_RNN_Glove200d.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":8,"outputs":[{"output_type":"stream","text":"Model: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 1000)]            0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 1000, 200)         1000000   \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 1000, 120)         31320     \n_________________________________________________________________\nglobal_max_pooling1d_1 (Glob (None, 120)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 16)                1936      \n_________________________________________________________________\ndense_3 (Dense)              (None, 5)                 85        \n=================================================================\nTotal params: 1,033,341\nTrainable params: 1,033,341\nNon-trainable params: 0\n_________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEpoch 1/10\n38/38 - 40s - loss: 1.5808 - accuracy: 0.3356 - val_loss: 1.4303 - val_accuracy: 0.4194\nEpoch 2/10\n38/38 - 40s - loss: 1.3190 - accuracy: 0.4637 - val_loss: 1.2299 - val_accuracy: 0.5206\nEpoch 3/10\n38/38 - 50s - loss: 1.1033 - accuracy: 0.5460 - val_loss: 1.0168 - val_accuracy: 0.5757\nEpoch 4/10\n38/38 - 39s - loss: 0.8695 - accuracy: 0.6527 - val_loss: 0.8339 - val_accuracy: 0.6982\nEpoch 5/10\n38/38 - 40s - loss: 0.6570 - accuracy: 0.7831 - val_loss: 0.6947 - val_accuracy: 0.7599\nEpoch 6/10\n38/38 - 41s - loss: 0.4802 - accuracy: 0.8581 - val_loss: 0.5851 - val_accuracy: 0.8002\nEpoch 7/10\n38/38 - 40s - loss: 0.3337 - accuracy: 0.9126 - val_loss: 0.4794 - val_accuracy: 0.8388\nEpoch 8/10\n38/38 - 41s - loss: 0.2131 - accuracy: 0.9531 - val_loss: 0.4208 - val_accuracy: 0.8635\nEpoch 9/10\n38/38 - 41s - loss: 0.1271 - accuracy: 0.9811 - val_loss: 0.3774 - val_accuracy: 0.8799\nEpoch 10/10\n38/38 - 41s - loss: 0.0796 - accuracy: 0.9912 - val_loss: 0.3660 - val_accuracy: 0.8914\n","name":"stdout"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7ff9b0528a50>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Simple RNN with Glove200D pretrained embeddings\n\nThe model architecture can be shown as below:\n\n<img src=\"https://i.imgur.com/3ZBQApl.png\">"},{"metadata":{},"cell_type":"markdown","source":"# LSTM- Long Short Term Memory\n\n[LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) are gated recurrent networks having 4 gates with (tanh/sigmoid) activation units. These architectures are the the building blocks of all the transformer architectures that we see, and the 4 gates combine input from different time stamps to produce the output. In a LSTM, there are typically 3 input and output signals: The h (hidden cell output from the previous timestep), c (the signal from previous cell), and the x(input vectors). Outputs involve the updated ht+1(hidden cell output of current block) value, ct+1, (updated c signal from the present cell) and the output(o).\n\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\">\n\n\n## Operation Steps - LSTM\n\nThe first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a sigmoid layer called the “forget gate layer.” It looks at ht−1 and xt, and outputs a number between 0 and 1 for each number in the cell state Ct−1. A 1 represents “completely keep this” while a 0 represents “completely get rid of this.”\n\nLet’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.\n\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\"> \n\n\nThe next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a sigmoid layer called the “input gate layer” decides which values we’ll update. Next, a tanh layer creates a vector of new candidate values, C~t, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.\n\nIn the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\">\n\nIt’s now time to update the old cell state, Ct−1, into the new cell state Ct. The previous steps already decided what to do, we just need to actually do it.\n\nWe multiply the old state by ft, forgetting the things we decided to forget earlier. Then we add it∗C~t. This is the new candidate values, scaled by how much we decided to update each state value.\n\nIn the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\">\n\nFinally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n\nFor the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.\n\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\">\n\n\n\nResources:\n\n- [NLP](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)\n- [Paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43905.pdf)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\n\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":9,"outputs":[{"output_type":"stream","text":"Model: \"functional_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         [(None, 1000)]            0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 1000, 768)         3840000   \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 1000, 120)         397920    \n_________________________________________________________________\nglobal_max_pooling1d_2 (Glob (None, 120)               0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 16)                1936      \n_________________________________________________________________\ndense_5 (Dense)              (None, 5)                 85        \n=================================================================\nTotal params: 4,239,941\nTrainable params: 4,239,941\nNon-trainable params: 0\n_________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEpoch 1/10\n38/38 - 248s - loss: 1.3345 - accuracy: 0.4464 - val_loss: 1.0801 - val_accuracy: 0.5617\nEpoch 2/10\n38/38 - 253s - loss: 0.8651 - accuracy: 0.6504 - val_loss: 0.7513 - val_accuracy: 0.6850\nEpoch 3/10\n38/38 - 246s - loss: 0.4942 - accuracy: 0.8094 - val_loss: 0.5875 - val_accuracy: 0.7747\nEpoch 4/10\n38/38 - 254s - loss: 0.2777 - accuracy: 0.9143 - val_loss: 0.4989 - val_accuracy: 0.8380\nEpoch 5/10\n38/38 - 247s - loss: 0.1544 - accuracy: 0.9607 - val_loss: 0.4958 - val_accuracy: 0.8520\nEpoch 6/10\n38/38 - 253s - loss: 0.0826 - accuracy: 0.9823 - val_loss: 0.4470 - val_accuracy: 0.8692\nEpoch 7/10\n38/38 - 257s - loss: 0.0376 - accuracy: 0.9936 - val_loss: 0.4811 - val_accuracy: 0.8668\nEpoch 8/10\n38/38 - 256s - loss: 0.0330 - accuracy: 0.9938 - val_loss: 0.4465 - val_accuracy: 0.8725\nEpoch 9/10\n38/38 - 264s - loss: 0.0256 - accuracy: 0.9944 - val_loss: 0.5554 - val_accuracy: 0.8610\nEpoch 10/10\n38/38 - 255s - loss: 0.0204 - accuracy: 0.9957 - val_loss: 0.5040 - val_accuracy: 0.8824\n","name":"stdout"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7ff97ae9b450>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## LSTM model architecture\n\n\nThe model architecture can be shown as below:\n\n\n<img src=\"https://i.imgur.com/81b4WSd.png\">"},{"metadata":{},"cell_type":"markdown","source":"## LSTM model with embeddings\n\nNow we will be applying the glove embeddings (200d) for boosting performance (if any)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using with pretrained Glove 200d embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=200\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding -with pretrained glove weights\n#3.Simple RNN- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_Glove200d.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":10,"outputs":[{"output_type":"stream","text":"Model: \"functional_7\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 1000)]            0         \n_________________________________________________________________\nembedding_3 (Embedding)      (None, 1000, 200)         1000000   \n_________________________________________________________________\nbidirectional_3 (Bidirection (None, 1000, 120)         125280    \n_________________________________________________________________\nglobal_max_pooling1d_3 (Glob (None, 120)               0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 16)                1936      \n_________________________________________________________________\ndense_7 (Dense)              (None, 5)                 85        \n=================================================================\nTotal params: 1,127,301\nTrainable params: 1,127,301\nNon-trainable params: 0\n_________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEpoch 1/10\n38/38 - 132s - loss: 1.3519 - accuracy: 0.4353 - val_loss: 1.1566 - val_accuracy: 0.5329\nEpoch 2/10\n38/38 - 120s - loss: 1.0282 - accuracy: 0.5661 - val_loss: 0.9622 - val_accuracy: 0.6020\nEpoch 3/10\n38/38 - 110s - loss: 0.8217 - accuracy: 0.6889 - val_loss: 0.8527 - val_accuracy: 0.6628\nEpoch 4/10\n38/38 - 99s - loss: 0.6448 - accuracy: 0.7648 - val_loss: 0.6526 - val_accuracy: 0.7516\nEpoch 5/10\n38/38 - 98s - loss: 0.4489 - accuracy: 0.8521 - val_loss: 0.5745 - val_accuracy: 0.7870\nEpoch 6/10\n38/38 - 98s - loss: 0.3353 - accuracy: 0.8957 - val_loss: 0.4852 - val_accuracy: 0.8215\nEpoch 7/10\n38/38 - 104s - loss: 0.2217 - accuracy: 0.9369 - val_loss: 0.4437 - val_accuracy: 0.8512\nEpoch 8/10\n38/38 - 99s - loss: 0.1599 - accuracy: 0.9591 - val_loss: 0.4048 - val_accuracy: 0.8627\nEpoch 9/10\n38/38 - 98s - loss: 0.1037 - accuracy: 0.9747 - val_loss: 0.3910 - val_accuracy: 0.8750\nEpoch 10/10\n38/38 - 99s - loss: 0.0717 - accuracy: 0.9852 - val_loss: 0.4094 - val_accuracy: 0.8890\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7ff9782588d0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## LSTM with Glove Embedding Architecture\n\nThe model architecture is as follows:\n\n<img src=\"https://i.imgur.com/oOmKx56.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Gated Recurrent Units\n\n[GRUs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) A slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single “update gate.” It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\n\n\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.GlobalMaxPooling (optional)\n#5.Dense Layer with Relu activation\n#6.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nz=Bidirectional(GRU(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_GRU.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":6,"outputs":[{"output_type":"stream","text":"Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 1000)]            0         \n_________________________________________________________________\nembedding (Embedding)        (None, 1000, 768)         3840000   \n_________________________________________________________________\nbidirectional (Bidirectional (None, 1000, 120)         298800    \n_________________________________________________________________\nglobal_max_pooling1d (Global (None, 120)               0         \n_________________________________________________________________\ndense (Dense)                (None, 16)                1936      \n_________________________________________________________________\ndense_1 (Dense)              (None, 5)                 85        \n=================================================================\nTotal params: 4,140,821\nTrainable params: 4,140,821\nNon-trainable params: 0\n_________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEpoch 1/10\n38/38 - 16s - loss: 1.4851 - accuracy: 0.3864 - val_loss: 1.3575 - val_accuracy: 0.4613\nEpoch 2/10\n38/38 - 15s - loss: 1.0679 - accuracy: 0.5994 - val_loss: 0.7618 - val_accuracy: 0.7747\nEpoch 3/10\n38/38 - 15s - loss: 0.4600 - accuracy: 0.8764 - val_loss: 0.5079 - val_accuracy: 0.8355\nEpoch 4/10\n38/38 - 15s - loss: 0.1813 - accuracy: 0.9624 - val_loss: 0.4120 - val_accuracy: 0.8742\nEpoch 5/10\n38/38 - 15s - loss: 0.0801 - accuracy: 0.9870 - val_loss: 0.3864 - val_accuracy: 0.8931\nEpoch 6/10\n38/38 - 15s - loss: 0.0347 - accuracy: 0.9967 - val_loss: 0.4354 - val_accuracy: 0.8914\nEpoch 7/10\n38/38 - 15s - loss: 0.0170 - accuracy: 0.9988 - val_loss: 0.4676 - val_accuracy: 0.8882\nEpoch 8/10\n38/38 - 15s - loss: 0.0106 - accuracy: 0.9996 - val_loss: 0.5000 - val_accuracy: 0.8873\nEpoch 9/10\n38/38 - 15s - loss: 0.0067 - accuracy: 0.9998 - val_loss: 0.5258 - val_accuracy: 0.8923\nEpoch 10/10\n38/38 - 15s - loss: 0.0051 - accuracy: 0.9998 - val_loss: 0.5446 - val_accuracy: 0.8857\n","name":"stdout"},{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f1a3da79ad0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture for vanilla GRU\n\nThe model architecture is as follows:\n\n<img src=\"https://i.imgur.com/jaZegBX.png\">"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion of Basics of Neural Network Models\n\nWe have seen a glimpse of LSTM/GRU along with Simple RNNs and also saw how to create a very simple classification neural network model with minimal lines of code , that too along with pretrained glove embeddings. Now we will be going into understanding about Transformer architectures."},{"metadata":{},"cell_type":"markdown","source":"# Understanding Attention Mechanism\n\n[Attention mechanism](https://arxiv.org/abs/1706.03762) is the most important aspect in language modelling. There are many variants of attention such as Bahdanau Attention, Luong Attention, Dot Product Attention,Self Attention. A detailed description of the attention mechanism is provided in this [kernel](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)\n\n\n## Bahdanau Attention\n\n\n<img src=\"https://miro.medium.com/max/639/1*qhOlQHLdtfZORIXYuoCtaA.png\">\n\n\nBahdanau et al. proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states.\n\nlet’s understand the Attention mechanism suggested by Bahdanau\n\nAll hidden states of the encoder(forward and backward) and the decoder are used to generate the context vector, unlike how just the last encoder hidden state is used in seq2seq without attention.\nThe attention mechanism aligns the input and output sequences, with an alignment score parameterized by a feed-forward network. It helps to pay attention to the most relevant information in the source sequence.\nThe model predicts a target word based on the context vectors associated with the source position and the previously generated target words.\nAlignment Score\nThe alignment score maps how well the inputs around position “j” and the output at position “i” match. The score is based on the previous decoder’s hidden state, s₍ᵢ₋₁₎ just before predicting the target word and the hidden state, hⱼ of the input sentence.\n\n\n<img src=\"https://miro.medium.com/max/535/1*u2YdTRPjN34Fpr-zxvoJsg.png\">\n\n\nThe decoder decides which part of the source sentence it needs to pay attention to, instead of having encoder encode all the information of the source sentence into a fixed-length vector. The alignment vector that has the same length with the source sequence and is computed at every time step of the decode.\n\nAttention Weights\nWe apply a softmax activation function to the alignment scores to obtain the attention weights.\n\n\n<img src=\"https://miro.medium.com/max/685/1*3aCyU9aSVHvxzOwvQdExdQ.png\">\n\n\n\n\n<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\">\n\n<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\">\n\n\n\n\n## Luong Attention\n\n\n\n\n### Global Attention\n\n\n<img src=\"https://miro.medium.com/max/626/1*LhEapXF1mtaB3rDgIjcceg.png\">\n\nLuong, et al., 2015 proposed the “global” and “local” attention. The global attention is similar to the soft attention, while the local one is an interesting blend between hard and soft, an improvement over the hard attention to make it differentiable: the model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector.\n\nThe commonality between Global and Local attention\n\n- At each time step t, in the decoding phase, both approaches, global and local attention, first take the hidden state hₜ at the top layer of a stacking LSTM as an input.\n- The goal of both approaches is to derive a context vector 𝒸ₜ to capture relevant source-side information to help predict the current target word yₜ\n- Attentional vectors are fed as inputs to the next time steps to inform the model about past alignment decisions.\n- Global and local attention models differ in how the context vector 𝒸ₜ is derived\n- Before we discuss the global and local attention, let’s understand the conventions used by Luong’s attention mechanism for any given time t\n\n   - 𝒸ₜ : context vector\n   - aₜ : alignment vector\n   - hₜ : current target hidden state\n   - hₛ : current source hidden state\n   - yₜ: predicted current target word\n   - h˜ₜ : Attentional vectors\n   \n   \nThe global attentional model considers all the hidden states of the encoder when calculating the context vector 𝒸ₜ.\n\nA variable-length alignment vector aₜ equal to the size of the number of time steps in the source sequence is derived by comparing the current target hidden state hₜ with each of the source hidden state hₛ\nThe alignment score is referred to as a content-based function for which we consider three different alternatives\n\n\n### Local Attention\n\n\n<img src=\"https://miro.medium.com/max/538/1*YXjdGl3CnSfHfzYpQiObgg.png\">\n\n\n\n- Local attention only focuses on a small subset of source positions per target words unlike the entire source sequence as in global attention\n- Computationally less expensive than global attention\n- The local attention model first generates an aligned position Pₜ for each target word at time t.\n- The context vector 𝒸ₜ is derived as a weighted average over the set of source hidden states within selected the window\n- The aligned position can be monotonically or predictively selected\n\n\n### Formulation\n\n\n<img src=\"https://miro.medium.com/max/875/1*_Ta67S8_lXTbVzJMztkxKg.png\">\n\n\n\n## List of Different Attention Mechanisms\n\n\n<img src=\"https://theaisummer.com/assets/img/posts/attention/attention-calculation.png\">\n\n\n## Resources:\n\n- [Luong Paper](https://arxiv.org/abs/1508.04025)\n- [Bahdanau Paper](https://arxiv.org/abs/1409.0473)\n- [Luong 2015](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf)\n- [Kernel](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)"},{"metadata":{},"cell_type":"markdown","source":"# Using a minimalistic implementation of Attention\n\nIn this scope, we will be building our own implementations of Attention mechanisms for our use case.Most of the implementations will have similar parameters. In the 'call' method, we generally have 2 vector inputs, but here we will be using a single input vector. The reason is because these algorithms are built for encoder-decoder models but in this case, since we only have a single neural network model , we need only one dimensional input vectors.\n\nHence, \n\n```python\ndef call(self,q):\n        self.q=q\n        self.v=q\n```\n\nWe are using the same q for both the inputs. In our sequential model, the q is the output from the, LSTM cell (only the outputs and no hidden cell states). Since the outputs/inputs of LSTM are 3 and for a GRU are 2, we have to modify the basic meural network model to adapt with the Attention mechanism.In most cases, the implementation of the model remains the same:\n\n- Creating Input layer\n- Creating the Embedding Layer (if pretrained embeddings are used or not)\n- Applying a LSTM/GRU variant or a bidirectional variant\n- Applying an Attention mechanism on top of the outputs\n- Feed Forward Dense Networks with the required activation functions\n\n\nWhen we will be going through a simple use case of Encoder Decoder based attention, then we will be using a 2d input based Attention mechanisms. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Implementing Different Attention Layers for our keras model\nimport math\nclass Bahdanau_Attention_1D(tf.keras.layers.Layer):\n    #A class for Bahdanau Attention\n    def __init__(self,units):\n        super(Bahdanau_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q):\n        self.q=q\n        self.v=q\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n        score=self.Wv(tf.nn.tanh(self.Wq(self.q)+self.Wk(self.v)))\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n\n\nclass Luong_Attention_1D(tf.keras.layers.Layer):\n    #A class for Luong Attention\n    def __init__(self,units):\n        super(Luong_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q):\n        self.q=q\n        self.v=q\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.q)*(self.v)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\nclass Scaled_Dot_Product_Attention_1D(tf.keras.layers.Layer):\n    #Scaled dot product Attention\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,n):\n        self.q=q\n        self.v=q\n        self.n=n\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=((self.q)*(self.v))/math.sqrt(self.n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.Attention- Applying 1D attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nBilstm_cell,_,_=(LSTM(60,return_state='True'))(z)\nbahdanau_attention=Bahdanau_Attention_1D(60)\n_,attention_weights_h=bahdanau_attention(Bilstm_cell)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_Bahdanau.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))","execution_count":8,"outputs":[{"output_type":"stream","text":"attention_weights Tensor(\"bahdanau__attention_1d/Softmax:0\", shape=(None, 60), dtype=float32)\nModel: \"functional_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 1000)]            0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 1000, 768)         3840000   \n_________________________________________________________________\nlstm (LSTM)                  [(None, 60), (None, 60),  198960    \n_________________________________________________________________\nbahdanau__attention_1d (Bahd ((None, 60), (None, 60))  10980     \n_________________________________________________________________\ndense_5 (Dense)              (None, 16)                976       \n_________________________________________________________________\ndense_6 (Dense)              (None, 5)                 85        \n=================================================================\nTotal params: 4,051,001\nTrainable params: 4,051,001\nNon-trainable params: 0\n_________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEpoch 1/10\n38/38 - 12s - loss: 1.5718 - accuracy: 0.3167 - val_loss: 1.5367 - val_accuracy: 0.4062\nEpoch 2/10\n38/38 - 12s - loss: 1.5137 - accuracy: 0.4004 - val_loss: 1.4972 - val_accuracy: 0.4062\nEpoch 3/10\n38/38 - 12s - loss: 1.4907 - accuracy: 0.4004 - val_loss: 1.4869 - val_accuracy: 0.4062\nEpoch 4/10\n38/38 - 12s - loss: 1.4709 - accuracy: 0.4004 - val_loss: 1.4560 - val_accuracy: 0.4062\nEpoch 5/10\n38/38 - 12s - loss: 1.4096 - accuracy: 0.4357 - val_loss: 1.4200 - val_accuracy: 0.4383\nEpoch 6/10\n38/38 - 12s - loss: 1.3317 - accuracy: 0.5172 - val_loss: 1.3359 - val_accuracy: 0.4860\nEpoch 7/10\n38/38 - 13s - loss: 1.1688 - accuracy: 0.5495 - val_loss: 1.2514 - val_accuracy: 0.5123\nEpoch 8/10\n38/38 - 12s - loss: 1.0588 - accuracy: 0.6329 - val_loss: 1.1071 - val_accuracy: 0.5962\nEpoch 9/10\n38/38 - 12s - loss: 0.8664 - accuracy: 0.6987 - val_loss: 1.0142 - val_accuracy: 0.6258\nEpoch 10/10\n38/38 - 12s - loss: 0.7681 - accuracy: 0.7162 - val_loss: 1.0097 - val_accuracy: 0.6488\n","name":"stdout"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f1a3dbfba50>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Model architecture of simple LSTM with Bahdanau Attention\n\nThe model architecture is as follows:\n\n<img src=\"https://i.imgur.com/a0q8CnD.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.Attention- Applying 1D attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nlstm_cell,_,_=(LSTM(60,return_state='True'))(z)\nluong_attention=Luong_Attention_1D(60)\n_,attention_weights_h=luong_attention(lstm_cell)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_Luong.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=20,verbose=2,validation_data=(val_x,val_y))","execution_count":9,"outputs":[{"output_type":"stream","text":"attention_weights Tensor(\"luong__attention_1d/Softmax:0\", shape=(None, 60), dtype=float32)\nModel: \"functional_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         [(None, 1000)]            0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 1000, 768)         3840000   \n_________________________________________________________________\nlstm_1 (LSTM)                [(None, 60), (None, 60),  198960    \n_________________________________________________________________\nluong__attention_1d (Luong_A ((None, 60), (None, 60))  0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 16)                976       \n_________________________________________________________________\ndense_11 (Dense)             (None, 5)                 85        \n=================================================================\nTotal params: 4,040,021\nTrainable params: 4,040,021\nNon-trainable params: 0\n_________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEpoch 1/20\n38/38 - 13s - loss: 1.5837 - accuracy: 0.3884 - val_loss: 1.5608 - val_accuracy: 0.4062\nEpoch 2/20\n38/38 - 12s - loss: 1.5374 - accuracy: 0.4004 - val_loss: 1.5165 - val_accuracy: 0.4062\nEpoch 3/20\n38/38 - 12s - loss: 1.5013 - accuracy: 0.4004 - val_loss: 1.4911 - val_accuracy: 0.4062\nEpoch 4/20\n38/38 - 12s - loss: 1.4797 - accuracy: 0.4004 - val_loss: 1.4684 - val_accuracy: 0.4062\nEpoch 5/20\n38/38 - 12s - loss: 1.4461 - accuracy: 0.4004 - val_loss: 1.4340 - val_accuracy: 0.4062\nEpoch 6/20\n38/38 - 12s - loss: 1.4052 - accuracy: 0.4004 - val_loss: 1.3973 - val_accuracy: 0.4062\nEpoch 7/20\n38/38 - 12s - loss: 1.3538 - accuracy: 0.4004 - val_loss: 1.3420 - val_accuracy: 0.4062\nEpoch 8/20\n38/38 - 12s - loss: 1.2940 - accuracy: 0.4004 - val_loss: 1.2955 - val_accuracy: 0.4062\nEpoch 9/20\n38/38 - 12s - loss: 1.2404 - accuracy: 0.4004 - val_loss: 1.2421 - val_accuracy: 0.4062\nEpoch 10/20\n38/38 - 12s - loss: 1.1855 - accuracy: 0.4004 - val_loss: 1.2013 - val_accuracy: 0.4062\nEpoch 11/20\n38/38 - 12s - loss: 1.1410 - accuracy: 0.4686 - val_loss: 1.1639 - val_accuracy: 0.5403\nEpoch 12/20\n38/38 - 12s - loss: 1.0963 - accuracy: 0.5556 - val_loss: 1.1315 - val_accuracy: 0.5362\nEpoch 13/20\n38/38 - 12s - loss: 1.0538 - accuracy: 0.5562 - val_loss: 1.1044 - val_accuracy: 0.5304\nEpoch 14/20\n38/38 - 12s - loss: 1.0165 - accuracy: 0.5564 - val_loss: 1.0709 - val_accuracy: 0.5312\nEpoch 15/20\n38/38 - 12s - loss: 0.9866 - accuracy: 0.5558 - val_loss: 1.0482 - val_accuracy: 0.5395\nEpoch 16/20\n38/38 - 12s - loss: 0.9522 - accuracy: 0.5573 - val_loss: 1.0282 - val_accuracy: 0.5378\nEpoch 17/20\n38/38 - 12s - loss: 0.9257 - accuracy: 0.5585 - val_loss: 1.0106 - val_accuracy: 0.5387\nEpoch 18/20\n38/38 - 12s - loss: 0.9034 - accuracy: 0.5581 - val_loss: 1.0060 - val_accuracy: 0.5345\nEpoch 19/20\n38/38 - 12s - loss: 0.8838 - accuracy: 0.5595 - val_loss: 0.9885 - val_accuracy: 0.5378\nEpoch 20/20\n38/38 - 12s - loss: 0.8633 - accuracy: 0.5608 - val_loss: 0.9763 - val_accuracy: 0.5411\n","name":"stdout"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f1a3dbffbd0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings\n\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.Attention- Applying 1D attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nlstm_cell,_,_=(LSTM(60,return_state='True'))(z)\nsdp_attention=Scaled_Dot_Product_Attention_1D(60)\n_,attention_weights_h=sdp_attention(lstm_cell,64)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_SDPAttention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=20,verbose=2,validation_data=(val_x,val_y))","execution_count":10,"outputs":[{"output_type":"stream","text":"attention_weights Tensor(\"scaled__dot__product__attention_1d/Softmax:0\", shape=(None, 60), dtype=float32)\nModel: \"functional_7\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 1000)]            0         \n_________________________________________________________________\nembedding_3 (Embedding)      (None, 1000, 768)         3840000   \n_________________________________________________________________\nlstm_2 (LSTM)                [(None, 60), (None, 60),  198960    \n_________________________________________________________________\nscaled__dot__product__attent ((None, 60), (None, 60))  0         \n_________________________________________________________________\ndense_15 (Dense)             (None, 16)                976       \n_________________________________________________________________\ndense_16 (Dense)             (None, 5)                 85        \n=================================================================\nTotal params: 4,040,021\nTrainable params: 4,040,021\nNon-trainable params: 0\n_________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEpoch 1/20\n38/38 - 12s - loss: 1.6016 - accuracy: 0.3479 - val_loss: 1.5911 - val_accuracy: 0.4062\nEpoch 2/20\n38/38 - 12s - loss: 1.5813 - accuracy: 0.4004 - val_loss: 1.5710 - val_accuracy: 0.4062\nEpoch 3/20\n38/38 - 12s - loss: 1.5598 - accuracy: 0.4004 - val_loss: 1.5492 - val_accuracy: 0.4062\nEpoch 4/20\n38/38 - 12s - loss: 1.5376 - accuracy: 0.4004 - val_loss: 1.5283 - val_accuracy: 0.4062\nEpoch 5/20\n38/38 - 12s - loss: 1.5183 - accuracy: 0.4004 - val_loss: 1.5117 - val_accuracy: 0.4062\nEpoch 6/20\n38/38 - 12s - loss: 1.5042 - accuracy: 0.4004 - val_loss: 1.5003 - val_accuracy: 0.4062\nEpoch 7/20\n38/38 - 12s - loss: 1.4956 - accuracy: 0.4004 - val_loss: 1.4938 - val_accuracy: 0.4062\nEpoch 8/20\n38/38 - 12s - loss: 1.4908 - accuracy: 0.4004 - val_loss: 1.4900 - val_accuracy: 0.4062\nEpoch 9/20\n38/38 - 12s - loss: 1.4877 - accuracy: 0.4004 - val_loss: 1.4877 - val_accuracy: 0.4062\nEpoch 10/20\n38/38 - 12s - loss: 1.4855 - accuracy: 0.4004 - val_loss: 1.4858 - val_accuracy: 0.4062\nEpoch 11/20\n38/38 - 12s - loss: 1.4840 - accuracy: 0.4004 - val_loss: 1.4849 - val_accuracy: 0.4062\nEpoch 12/20\n38/38 - 12s - loss: 1.4835 - accuracy: 0.4004 - val_loss: 1.4835 - val_accuracy: 0.4062\nEpoch 13/20\n38/38 - 12s - loss: 1.4825 - accuracy: 0.4004 - val_loss: 1.4830 - val_accuracy: 0.4062\nEpoch 14/20\n38/38 - 12s - loss: 1.4820 - accuracy: 0.4004 - val_loss: 1.4823 - val_accuracy: 0.4062\nEpoch 15/20\n38/38 - 12s - loss: 1.4811 - accuracy: 0.4004 - val_loss: 1.4815 - val_accuracy: 0.4062\nEpoch 16/20\n38/38 - 12s - loss: 1.4803 - accuracy: 0.4004 - val_loss: 1.4810 - val_accuracy: 0.4062\nEpoch 17/20\n38/38 - 12s - loss: 1.4795 - accuracy: 0.4004 - val_loss: 1.4801 - val_accuracy: 0.4062\nEpoch 18/20\n38/38 - 12s - loss: 1.4789 - accuracy: 0.4004 - val_loss: 1.4798 - val_accuracy: 0.4062\nEpoch 19/20\n38/38 - 12s - loss: 1.4782 - accuracy: 0.4004 - val_loss: 1.4789 - val_accuracy: 0.4062\nEpoch 20/20\n38/38 - 12s - loss: 1.4774 - accuracy: 0.4004 - val_loss: 1.4781 - val_accuracy: 0.4062\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f1a3c522690>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Creating a 1 Layer Encoder Decoder Transformer with Self Attention\n\n\n\n## Self Attention Mechanism\n\n\nSelf Attention is a mechanism for specifying the the attention on parts of a sentence to retain more semantic information. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing. [Jay's Blog](http://jalammar.github.io/illustrated-transformer/) provide a very good idea of this logic.\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\">\n\nThree vectors q,k and v (query,key and value) are taken into consideration for computation of the self attention mechanism.The q,k and v are normally of 64 dimensions.\n\n\n<img src=\"http://jalammar.github.io/images/t/transformer_self_attention_vectors.png\">\n\nThe score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\n\n<img src=\"http://jalammar.github.io/images/t/transformer_self_attention_score.png\">\n\n\nThe third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.\n\n\n<img src=\"http://jalammar.github.io/images/t/self-attention_softmax.png\">\n\n\nThis softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word. The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example). The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n\n<img src=\"http://jalammar.github.io/images/t/self-attention-output.png\">\n\n\nThe computation process for Self Attention can be regarded as follows:\n\n\n<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Creating 1D self Attention Mechanism for LSTM networks\n\nBefore moving to Encoder Decoder Models, we can try applying a Self Attention mechanism on the standard network that we have created."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass Scaled_Dot_Product_Self_Attention_1D(tf.keras.layers.Layer):\n    #A class for Self Attention- 1 Dimension\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention_1D,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,n):\n        self.q=q\n        self.v=q\n        self.n=n\n        self.k=q\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    #A class for Self Attention- Q,K,V dimensions\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n#Important parameters when using without pretrained embeddings\nmaxlen=1000\nmax_features=5000 \nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency\n#4.Attention- Applying 1D attention-Self Attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,input_length=maxlen)(inp)\nBilstm_cell,_,_=(LSTM(60,return_state='True'))(z)\nself_attention=Scaled_Dot_Product_Self_Attention_1D(60)\n_,attention_weights_h=self_attention(Bilstm_cell,64)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_LSTM_Self.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))\n","execution_count":11,"outputs":[{"output_type":"stream","text":"attention_weights Tensor(\"scaled__dot__product__self__attention_1d/Softmax:0\", shape=(None, 60), dtype=float32)\nModel: \"functional_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_5 (InputLayer)         [(None, 1000)]            0         \n_________________________________________________________________\nembedding_4 (Embedding)      (None, 1000, 768)         3840000   \n_________________________________________________________________\nlstm_3 (LSTM)                [(None, 60), (None, 60),  198960    \n_________________________________________________________________\nscaled__dot__product__self__ ((None,), (None, 60))     7320      \n_________________________________________________________________\ndense_20 (Dense)             (None, 16)                976       \n_________________________________________________________________\ndense_21 (Dense)             (None, 5)                 85        \n=================================================================\nTotal params: 4,047,341\nTrainable params: 4,047,341\nNon-trainable params: 0\n_________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEpoch 1/10\n38/38 - 12s - loss: 1.5920 - accuracy: 0.3603 - val_loss: 1.5712 - val_accuracy: 0.4062\nEpoch 2/10\n38/38 - 12s - loss: 1.5516 - accuracy: 0.4004 - val_loss: 1.5247 - val_accuracy: 0.4062\nEpoch 3/10\n38/38 - 12s - loss: 1.5004 - accuracy: 0.4004 - val_loss: 1.4868 - val_accuracy: 0.4062\nEpoch 4/10\n38/38 - 12s - loss: 1.4864 - accuracy: 0.4004 - val_loss: 1.4863 - val_accuracy: 0.4062\nEpoch 5/10\n38/38 - 12s - loss: 1.4857 - accuracy: 0.4004 - val_loss: 1.4845 - val_accuracy: 0.4062\nEpoch 6/10\n38/38 - 12s - loss: 1.4640 - accuracy: 0.4004 - val_loss: 1.4275 - val_accuracy: 0.4062\nEpoch 7/10\n38/38 - 12s - loss: 1.3803 - accuracy: 0.4004 - val_loss: 1.3795 - val_accuracy: 0.4062\nEpoch 8/10\n38/38 - 12s - loss: 1.3301 - accuracy: 0.4004 - val_loss: 1.3492 - val_accuracy: 0.4062\nEpoch 9/10\n38/38 - 12s - loss: 1.3044 - accuracy: 0.4004 - val_loss: 1.4560 - val_accuracy: 0.4062\nEpoch 10/10\n38/38 - 12s - loss: 1.3511 - accuracy: 0.4004 - val_loss: 1.3755 - val_accuracy: 0.4062\n","name":"stdout"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f19b1d71a10>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings-Glove 200 D and Self Attention on GRU\nmaxlen=1000\nmax_features=5000 \nembed_size=200\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency(Glove 200D)\n#4.Attention- Applying 1D attention-Self Attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\ngru_cell,_=(GRU(60,return_state='True'))(z)\nself_attention=Scaled_Dot_Product_Self_Attention_1D(60)\n_,attention_weights_h=self_attention(gru_cell,64)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_GRU_Self_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Using embedding matrices for semantic reference\n\nThe entire attention concepts can be applied with any embedding and in this case, we will be looking how to create transformer embeddings using minimal lines of code and use that along with the Attention mechanisms.\nWe have  already seen how to create Glove pretrained embeddings and using it; and as such any static pretrained embedding can be used for that purpose- be it - Fasttext/Paragram etc. Now we will be using Transformer embeddings for our use case. But before let us try to plot some word embeddings with the help of the state of the art Transformers.\n"},{"metadata":{},"cell_type":"markdown","source":"# Enter Transformers\n\nWe will be working with the [HuggingFace](https://huggingface.co/) repository as it contains all SOTA Transformer models. In this context, it is useful to mention some important resources:\n\n- Transformer Keras(https://keras.io/examples/nlp/text_classification_with_transformer/)\n- Kaggle Kernel(https://www.kaggle.com/suicaokhoailang/lstm-attention-baseline-0-652-lb)\n\nHowever in this case, since we would be using the models just for extracting embeddings or features, it is important to know the intermediate layers which should be chosen. Since Transformer architectures are really huge, (BERT/GPT variants), it is very complicated to fully understand which layer should be extracted for the features. While BERT, the first Transformer, relies on 2 tokens ([CLS] and [SEP]) ,extracting the sentence embedding vectors are done after extracting the last output layer. However , different models have different number of layers, and in this case, we will exploring a model agnostic way to extract sentence embeddings and performing similarity check with all of the models.\n\n\n\n<img src=\"http://jalammar.github.io/images/bert-next-sentence-prediction.png\">\n\n\n## BERT Embeddings\n\n[BERT](https://arxiv.org/abs/1810.04805) is a traditional SOTA transformer architecture published by Google Research which uses bidirectional pretraining . The importance of using BERT is that it has 2 important aspects:\n\nMsked Language Model (MLM)\nNext Sentence Prediction(NSP)\nThe bidirectional pre-training is essentially helpful to be used for any tasks. The Huggingface implementation is helpful for fine-tuning BERT for any language modelling task. The BERT architecture falls under an encoder-decoder(Transformer) model as follows:\n\n\n<img src=\"https://miro.medium.com/max/876/0*ViwaI3Vvbnd-CJSQ.png\">\n\n<img src=\"https://d3i71xaburhd42.cloudfront.net/df2b0e26d0599ce3e70df8a9da02e51594e0e992/15-Figure4-1.png\">\n\n\n## Extracting Embeddings from BERT variant Transformers\n\nFor finetuning, it is to be kept in mind, there are many ways to do this. We are using BERT from Huggingface repository while it can also be used from [TF-HUB](https://tfhub.dev/s?module-type=text-embedding) or from [Google-Research](https://github.com/google-research/bert) repository. The reason for using HuggingFace is that the same codebase is applicable for all language models. The 3 most important input features that any language model asks for is:\n\n- input_ids\n- attention_masks\n- token_ids\n\nLet us first try to analyse and understand how BERT tokenizers, and model can be used in this context. The BERT documentation provides an outline of how to use BERT tokenizers and also modify it for downstream tasks.\n\nGenerally by virtue of transfer learning through weight transfer, we use pretrained BERT models from the list. This allows us to finetune it to extract only the embeddings. Since we are using Keras, we have to build up a small model containing an Input Layer and apply the tokenized(encoded) input ids, attention masks as input to the pretrained and loaded BERT model.This is very similar to creating a very own classification model for BERT using Keras/Tensorflow, but since we will be needing only the Embeddings it is safe to extract only the sentence vectors in the last layer of the model output. In most of the cases , we will see that the dimensions of the output vector is (x,768) where x depends on the number of tokenized input features. For this we extract the [CLS] tokenized feature from the ouput to just extract the sentence embeddings.\n\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\">\n\n\nSo the following pattern is to be done for our use case:\n\n- First Attain BERT embeddings by capturing the last hidden state output\n- Create the standard Neural network model (which we created till now)\n- In the Embedding Layer fit the BERT embeddings\n- Apply Self-Attention/any Attention mechanism on BERT embeddings\n- Apply FFNN Dense Networks with required activation functions\n\nby BERT , I mean all transformer models  under huggingface [pretrained library](https://huggingface.co/transformers/pretrained_models.html)\n\n\n\nSome resources and source codes:\n\n- [My NLP Kernels](https://kaggle.com/abhilash1910)\n- [Extensive Word Embeddings with Distilbert/Roberta/XL NET](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop#BERT-Embeddings-with-Alternate-Strategy)\n- [NLP workshop-2](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india)"},{"metadata":{},"cell_type":"markdown","source":"## Generate any word pair similarity with Transformer (BERT variants)\n\nHere we will see how to create word pair similarity with very minimal lines of code using pretrained huge models like BERT,Distilbert,Roberta,XLNET,Camembert,BART,GPT etc."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Generate word by word embeddings with any BERT variant transformer models\n\nfrom transformers import BertTokenizer, TFBertModel\nfrom scipy.spatial.distance import cosine\n\ndef get_embeddings(model_name,tokenizer,name,inp):\n    #Specify which transformer model will be used\n    tokenizer = tokenizer.from_pretrained(name)\n    model = model_name.from_pretrained(name)\n    input_ids = tf.constant(tokenizer.encode(inp))[None, :]  # Batch size 1\n    outputs = model(input_ids)\n    #Take the last output\n    last_hidden_states = outputs[0]\n    cls_token=last_hidden_states[0]\n    return cls_token\n\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the BERT word embeddings\ncls_token1=get_embeddings(TFBertModel,BertTokenizer,'bert-base-uncased',input_word1)\ncls_token2=get_embeddings(TFBertModel,BertTokenizer,'bert-base-uncased',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1[0],cls_token2[0])\nprint('Word Pair Similarity',distance)\n#Plot the distance\nplt.plot(cls_token1[0])\nplt.plot(cls_token2[0])\n\n","execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9321d12ba7441a8be52efa04d1e1d1e"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24623ac38fd04669b7b75ab646e178be"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=536063208.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29d2367b5a0c4678b7d832d52263d58b"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\nSome layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","name":"stderr"},{"output_type":"stream","text":"Word Pair Similarity 0.8617103099822998\n","name":"stdout"},{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f19b1282d50>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## DistilBERT Embeddings\n\n[This](https://huggingface.co/transformers/model_doc/distilbert.html) is a distilled version of pretraining BERT to produce a lightweight version of it. It is analogous to teacher supervision of a neural network learning to optimize tis weights. DistilBERT Paper provides an insight why it is 40% smaller but preserves 95% of BERT's weights for transfer learning.\n\n\n<img src=\"https://storage.googleapis.com/groundai-web-prod/media%2Fusers%2Fuser_14%2Fproject_391208%2Fimages%2FKD_figures%2Ftransformer_distillation.png\">\n\nThe overall workflow is similar to BERT extracting BERT word embeddings\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-input-to-output-tensor-recap.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#DistilBERT word Embeddings\n\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the DistilBERT word embeddings\ncls_token1=get_embeddings(transformers.TFDistilBertModel,BertTokenizer,'distilbert-base-uncased',input_word1)\ncls_token2=get_embeddings(transformers.TFDistilBertModel,BertTokenizer,'distilbert-base-uncased',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1[0],cls_token2[0])\nprint('Word Pair Similarity',distance)\n#Plot the distance\nplt.plot(cls_token1[0])\nplt.plot(cls_token2[0])\n\n","execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d32790f59d9492ba940ba614585f1ff"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37b388cb72934425b9b80a2c75161d0b"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"256345ad5b3048ae9f8423e49009f47b"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2da0117e99a6425682f1fd0edfe24f90"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ef0b99c450643aea9de653321d7e879"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\nSome layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n","name":"stderr"},{"output_type":"stream","text":"Word Pair Similarity 0.9600869417190552\n","name":"stdout"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f15f56eecd0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## XL NET Embeddings\n\n[This paper](https://arxiv.org/abs/1906.08237) provides an important outline of the modifications made on top of BERT for producing XLNet. It applies an autoregressive language model and has the 2 most important points:\n\n- Enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order\n- Overcomes the limitations of BERT thanks to its autoregressive formulation.\n- It is a permutation language model and a pictorial representation can be :\n\n<img src=\"https://zdnet2.cbsistatic.com/hub/i/r/2019/06/21/2a4e6548-9dee-491d-b638-8cfae9bbb2fe/resize/1200x900/ab279544c2631111754a357ada50ef29/google-xlnet-architecture-2019.png\">\n\n\n\nHere we will be using an alternate strategy for building the word embeddings\nWe will be using the Feature Extraction Pipeline from Huggingface- just to show that there are more than one ways of retrieving the embeddings."},{"metadata":{"trusted":true},"cell_type":"code","source":"#XLNET word Embeddings\n#Using the Feature Extraction pipeline from Huggingface\n\nfrom transformers import AutoTokenizer, pipeline\nfrom scipy.spatial.distance import cosine\ndef transformer_embedding(model_name,name,inp):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\n\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the XLNET word embeddings\ncls_token1=transformer_embedding(transformers.TFXLNetModel,'xlnet-base-cased',input_word1)\ncls_token2=transformer_embedding(transformers.TFXLNetModel,'xlnet-base-cased',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1[0],cls_token2[0])\nprint('Word Pair Similarity',distance)\n#Plot the distance\nplt.plot(cls_token1[0])\nplt.plot(cls_token2[0])\n\n","execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72bd6f917cdb49da98136894d998e6ea"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=565485600.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"796638de1cb147ff8ecf6b9bf51c7d09"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetModel: ['lm_loss']\n- This IS expected if you are initializing TFXLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFXLNetModel were initialized from the model checkpoint at xlnet-base-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetModel for predictions without further training.\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"746bba2b02f34c6488697d308f126f54"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetModel: ['lm_loss']\n- This IS expected if you are initializing TFXLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFXLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFXLNetModel were initialized from the model checkpoint at xlnet-base-cased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetModel for predictions without further training.\n","name":"stderr"},{"output_type":"stream","text":"Word Pair Similarity 0.9300533971368935\n","name":"stdout"},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f19b108efd0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## Exploring Generative Transformers - GPT2\n\n\n<img src=\"http://jalammar.github.io/images/gpt2/openAI-GPT-2-3.png\">\n\n\nIt is a [robust model](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.Some important aspects:\n\nGPT-2 is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather than the left.\n\nGPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n\nGPT-2 is a decoder Transformer model. Generally most transformers are encoder-decoders, but in the case of GPT-2 this is a decoder-only model. GPT-2 has stacks of decoder cells on top of one another, and inside each decoder block resides -Masked Self Attention and FFNN (Dense) Networks.\n\n\n<img src=\"http://jalammar.github.io/images/gpt2/gpt2-self-attention-qkv-1-2.png\">\n\n\n\n## Masked Self Attention\n\n\nThis is the core part which separated GPT from BERT variants:\n\n\n<img src=\"http://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png\">\n\n\nMore details can be found here:\n\n- [NLP](https://www.kaggle.com/colearninglounge/nlp-end-to-end-cll-nlp-workshop-2)\n- [NLP](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india)\n- [Jay's blog](http://jalammar.github.io/illustrated-gpt2/)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#For GPT variants, it is important to know that these use 'PAD' tokens additionally and are used from left to right.\n#Unlike BERT variants which are bidirectional in nature, GPT is more of a left to right tokenizer due to the Masking of Attention.\n\ndef transformer_gpt_embedding(model_name,name,inp):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    tokenizer.pad_token = \"[PAD]\"\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\n\ninput_word1='playing'\ninput_word2='photography'\n#Retrieve the GPT-2 word embeddings\ncls_token1=transformer_gpt_embedding(transformers.TFGPT2Model,'openai-gpt',input_word1)\ncls_token2=transformer_gpt_embedding(transformers.TFGPT2Model,'openai-gpt',input_word2)\n\n#Measure the distance between the embeddings\ndistance=1-cosine(cls_token1,cls_token2)\nprint('Word Pair Similarity',distance)\n#Plot the distance\nplt.plot(cls_token1)\nplt.plot(cls_token2)\n\n","execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=656.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bcf344b20ed46edb2105da1b3e9bd42"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466312920.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f60a9873abb4cbab48ed42ae34e8b49"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"Some layers from the model checkpoint at openai-gpt were not used when initializing TFGPT2Model: ['positions_embed/embeddings:0', 'tokens_embed/weight:0']\n- This IS expected if you are initializing TFGPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFGPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFGPT2Model were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['wpe/embeddings:0', 'ln_f/gamma:0', 'ln_f/beta:0', 'wte/weight:0']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=815973.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1d21ae262804db09a4eef18269b00c3"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=458495.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bbdced5166d41c99acba7a2e751aa12"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\nSome layers from the model checkpoint at openai-gpt were not used when initializing TFGPT2Model: ['positions_embed/embeddings:0', 'tokens_embed/weight:0']\n- This IS expected if you are initializing TFGPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFGPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFGPT2Model were not initialized from the model checkpoint at openai-gpt and are newly initialized: ['wpe/embeddings:0', 'ln_f/gamma:0', 'ln_f/beta:0', 'wte/weight:0']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n","name":"stderr"},{"output_type":"stream","text":"Word Pair Similarity 0.8902670354347892\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f15f545a3d0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Take any word pair from the provided question body for measuring distance similarity.\ntrain_df['question_body'][0]","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"'After playing around with macro photography on-the-cheap (read: reversed lens, rev. lens mounted on a straight lens, passive extension tubes), I would like to get further with this. The problems with the techniques I used is that focus is manual and aperture control is problematic at best. This limited my setup to still subjects (read: dead insects) Now, as spring is approaching, I want to be able to shoot live insects. I believe that for this, autofocus and settable aperture will be of great help.\\n\\nSo, one obvious but expensive option is a macro lens (say, EF 100mm Macro) However, I am not really interested in yet another prime lens. An alternative is the electrical extension tubes.\\n\\nExcept for maximum focusing distance, what am I losing when using tubes (coupled with a fine lens, say EF70-200/2.8) instead of a macro lens?\\n'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Any Transformer Model can be used for our Classification Use case\n\n\nIn this case, we will be creating a function to create an embedding matrix (instead of word vectors) , very much like Glove. Important point is to remember that, this code is compatible with any BERT variant transformers and also the embedding dimensions should be compatible with the Transformer size mentioned in the [pretrained section](https://huggingface.co/transformers/pretrained_models.html)\n\nThe steps are straightforward, and it is as follows:\n\n- Batch tokenize the input features\n- Once these have been tokenized (with Transformer tokenizers) , we will be applying certain masks\n- Padding the tokenized text \n- Then we have to apply an attention mask\n- The attention mask signifies that we have to segregate the input features in 0s and 1s.\n- Extract the last hidden outputs \n\nUsing these embeddings, we can plug them into our standard neural network architecture (along with Attention)."},{"metadata":{"trusted":true},"cell_type":"code","source":"def chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n        \ndef fetch_vectors(string_list,pretrained_model,batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFDistilBertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        #This is the attention mask\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n        #Extract the last hidden states.\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\ndistilbert_embeddings = fetch_vectors(train_df.question_body.values,'distilbert-base-uncased')\nprint(distilbert_embeddings.shape)\nplt.plot(distilbert_embeddings[0])","execution_count":17,"outputs":[{"output_type":"stream","text":"Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_transform', 'activation_13', 'vocab_projector', 'vocab_layer_norm']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\nToken indices sequence length is longer than the specified maximum sequence length for this model (1414 > 512). Running this sequence through the model will result in indexing errors\n","name":"stderr"},{"output_type":"stream","text":"(6079, 768)\n","name":"stdout"},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f19b18110d0>]"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"## Apply the Older GRU with Self Attention Model on DistilBert Embeddings\n\nNow, we can apply the GRU model which we created on the DistilBert Embeddings and evaluate the classification performance.Important point to note, that here the dimension is 768, and the 6079 includes the number of features (max_features). We have to replace those variables accordingly.\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-model-input-output-1.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings-Distilbert and Self Attention on GRU\nmaxlen=1000\n#Obsserve the max_features which is -> distilbert_embeddings.shape[0]\nmax_features=6079\n#Observe the embed_size->distilbert_embeddings.shape[1]\nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency(Distilbert)\n#4.Attention- Applying 1D attention-Self Attention\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[distilbert_embeddings])(inp)\ngru_cell,_=(GRU(60,return_state='True'))(z)\nself_attention=Scaled_Dot_Product_Self_Attention_1D(60)\n_,attention_weights_h=self_attention(gru_cell,64)\nprint('attention_weights',attention_weights_h)\nz=attention_weights_h\n# z=GlobalMaxPool1D()(final_attention_weights)\nz=Dense(16,activation='relu')(z)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_GRU_Self_attention_Distilbert.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))\n","execution_count":18,"outputs":[{"output_type":"stream","text":"attention_weights Tensor(\"scaled__dot__product__self__attention_1d_1/Softmax:0\", shape=(None, 60), dtype=float32)\nModel: \"functional_11\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_6 (InputLayer)         [(None, 1000)]            0         \n_________________________________________________________________\nembedding_5 (Embedding)      (None, 1000, 768)         4668672   \n_________________________________________________________________\ngru_1 (GRU)                  [(None, 60), (None, 60)]  149400    \n_________________________________________________________________\nscaled__dot__product__self__ ((None,), (None, 60))     7320      \n_________________________________________________________________\ndense_25 (Dense)             (None, 16)                976       \n_________________________________________________________________\ndense_26 (Dense)             (None, 5)                 85        \n=================================================================\nTotal params: 4,826,453\nTrainable params: 4,826,453\nNon-trainable params: 0\n_________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEpoch 1/10\n38/38 - 13s - loss: 1.5790 - accuracy: 0.3572 - val_loss: 1.5424 - val_accuracy: 0.4062\nEpoch 2/10\n38/38 - 12s - loss: 1.5143 - accuracy: 0.4004 - val_loss: 1.4915 - val_accuracy: 0.4062\nEpoch 3/10\n38/38 - 12s - loss: 1.4884 - accuracy: 0.4004 - val_loss: 1.4857 - val_accuracy: 0.4062\nEpoch 4/10\n38/38 - 12s - loss: 1.4864 - accuracy: 0.4004 - val_loss: 1.4856 - val_accuracy: 0.4062\nEpoch 5/10\n38/38 - 12s - loss: 1.4859 - accuracy: 0.4004 - val_loss: 1.4854 - val_accuracy: 0.4062\nEpoch 6/10\n38/38 - 13s - loss: 1.4848 - accuracy: 0.4004 - val_loss: 1.4829 - val_accuracy: 0.4062\nEpoch 7/10\n38/38 - 12s - loss: 1.4792 - accuracy: 0.4004 - val_loss: 1.4736 - val_accuracy: 0.4062\nEpoch 8/10\n38/38 - 12s - loss: 1.4454 - accuracy: 0.4004 - val_loss: 1.4184 - val_accuracy: 0.4062\nEpoch 9/10\n38/38 - 12s - loss: 1.3432 - accuracy: 0.4006 - val_loss: 1.3257 - val_accuracy: 0.4252\nEpoch 10/10\n38/38 - 12s - loss: 1.2305 - accuracy: 0.4723 - val_loss: 1.2565 - val_accuracy: 0.4671\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f1a3dbad690>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Voila, we have added 2 major parts in our code- Transformer embeddings & Self Attention\n\n\nNow let us see how the performance is when we use just the Distilbert Embeddings for our use case, and no self attention /attention mechanism. We will see a steeper rise in the accuracy, this is because distilbert already has 12 transformer blocks with multihead self attention being plugged into it."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Important parameters when using without pretrained embeddings-Distilbert and Self Attention on GRU\nmaxlen=1000\n#Obsserve the max_features which is -> distilbert_embeddings.shape[0]\nmax_features=6079\n#Observe the embed_size->distilbert_embeddings.shape[1]\nembed_size=768\n\n\n#Desing a simple model\n#Layers:\n#1.Input\n#2.Embedding\n#3.Simple LSTM- With Bidirectionality to increase efficiency(Distilbert)\n#4.Attention- Applying 1D attention-Self Attention(Not applicable here)\n#5.GlobalMaxPooling (optional)\n#6.Dense Layer with Relu activation\n#7.Final Dense layer containing the input units = (no of unique labels in the corpus).In this case 5.\n\ninp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[distilbert_embeddings])(inp)\ngru_cell,_=(GRU(60,return_state='True'))(z)\nz=Dense(16,activation='relu')(gru_cell)\nz=Dense(5,activation='softmax')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"Simple_GRU_Distilbert.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\n#Split the training and test datasets\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#Run the model with the dataset with 128 batch size ,10 epochs and validation data.\nmodel.fit(train_x,train_y,batch_size=128,epochs=10,verbose=2,validation_data=(val_x,val_y))\n","execution_count":19,"outputs":[{"output_type":"stream","text":"Model: \"functional_13\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_7 (InputLayer)         [(None, 1000)]            0         \n_________________________________________________________________\nembedding_6 (Embedding)      (None, 1000, 768)         4668672   \n_________________________________________________________________\ngru_2 (GRU)                  [(None, 60), (None, 60)]  149400    \n_________________________________________________________________\ndense_27 (Dense)             (None, 16)                976       \n_________________________________________________________________\ndense_28 (Dense)             (None, 5)                 85        \n=================================================================\nTotal params: 4,819,133\nTrainable params: 4,819,133\nNon-trainable params: 0\n_________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEpoch 1/10\n38/38 - 13s - loss: 1.5725 - accuracy: 0.1692 - val_loss: 1.5573 - val_accuracy: 0.1587\nEpoch 2/10\n38/38 - 12s - loss: 1.4950 - accuracy: 0.3245 - val_loss: 1.4560 - val_accuracy: 0.4062\nEpoch 3/10\n38/38 - 12s - loss: 1.3711 - accuracy: 0.4037 - val_loss: 1.3363 - val_accuracy: 0.4260\nEpoch 4/10\n38/38 - 12s - loss: 1.0911 - accuracy: 0.5361 - val_loss: 1.1055 - val_accuracy: 0.5107\nEpoch 5/10\n38/38 - 12s - loss: 0.7335 - accuracy: 0.6990 - val_loss: 0.8555 - val_accuracy: 0.6538\nEpoch 6/10\n38/38 - 12s - loss: 0.5367 - accuracy: 0.7884 - val_loss: 0.7537 - val_accuracy: 0.7212\nEpoch 7/10\n38/38 - 12s - loss: 0.3896 - accuracy: 0.8702 - val_loss: 0.7676 - val_accuracy: 0.7360\nEpoch 8/10\n38/38 - 12s - loss: 0.2590 - accuracy: 0.9221 - val_loss: 0.7512 - val_accuracy: 0.7574\nEpoch 9/10\n38/38 - 12s - loss: 0.1767 - accuracy: 0.9513 - val_loss: 0.6965 - val_accuracy: 0.8043\nEpoch 10/10\n38/38 - 12s - loss: 0.0924 - accuracy: 0.9801 - val_loss: 0.7098 - val_accuracy: 0.8191\n","name":"stdout"},{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f1a3da6f190>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Fitting a Simple Traditional Classifier on DistilBERT\n\nNow we will try to fit a simple Logistic Regression function on the Distilbert Embeddings and evaluate the performance\n\n\n<img src=\"http://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification-example.png\">\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=train_y\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n#Building a simple logistic regression classifier\nmodel=LogisticRegression()\nmodel.fit(train_x,train_y)\npred=model.predict(val_x)\nprint(\"Evaluate confusion matrix for LR\")\nprint(confusion_matrix(val_y,pred))\nprint(f\"Accuracy Score for LR with C=1.0  ={accuracy_score(val_y,pred)}\")","execution_count":20,"outputs":[{"output_type":"stream","text":"Padded and Tokenized Training Sequence (4863, 1000)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 1000)\nTarget Values Shape (1216,)\nEvaluate confusion matrix for LR\n[[ 70   1  18  10  94]\n [  8  33  10  22  78]\n [ 15   7  25  19  74]\n [ 20  10  21  68 119]\n [ 51  24  23  48 348]]\nAccuracy Score for LR with C=1.0  =0.4473684210526316\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Why the poor performance?\n\nThis is because the problem requires a multi-class classification rather than a sigmoid binary classification. We need to try another model ,let's say XGBoost?"},{"metadata":{"trusted":true},"cell_type":"code","source":"#XGBoosting\n\nfrom xgboost import XGBClassifier as xg\nmodel_xgb= xg(n_estimators=100,random_state=42)\nmodel_xgb.fit(train_x,train_y)\ny_pred_xgb=model_xgb.predict(val_x)\nprint('Accuracy Score',accuracy_score(val_y,y_pred_xgb.round()))","execution_count":21,"outputs":[{"output_type":"stream","text":"Accuracy Score 0.7861842105263158\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Creating a 1 block Transformer\n\nNow, we will be using the same concept of distilbert embeddings and create a simple encoder-decoder with Attention block which is also known as a 1 block Transformer. [Encoder-Decoder](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) is a classic architecture mostly popular for sequence2sequence learning. Encoder-Decoders are most popularly used for neural machine translation (seq2seq learning with attention). The general workflow revolves around stacks of RNNs (LSTMs/GRUs/TimeDistributed Cells) which behaves as an encoder takes as input 3 parameters (max_features,embed_size,maxlen in our example) and returns an output. We then save the 2 output LSTM cell states ,the h and c states. We design the decoder model in a similar manner (if the internal layers are modified it becomes a hybrid decoder). And while passing the inputs of the decoder, we also pass the 2 output LSTM cell states from the encoder output (namely the h and c states). The output of the decoder is then passed through a activation/distribution function to optimize our target loss function.\n\n\n<img src=\"https://miro.medium.com/max/1250/1*LYGO4IxqUYftFdAccg5fVQ.png\">\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final 1 Block Transformer which consists of 1 encoder decoder with distilbert embedding\nimport math\nmaxlen=500\nembed_size=768\n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    #A class for Self Attention- Q,K,V dimensions\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n        \ndef fetch_vectors(string_list,pretrained_model,batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFDistilBertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        #This is the attention mask\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n        #Extract the last hidden states.\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\n\n\ndef distilbert_encoder_decoder_attention(maxlen,max_features,distilbert_embeddings):\n    #Creating LSTM  encoder neural model with distilbert pretrained embeddings\n    #Encoder Block\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(distilbert_embeddings.shape[0],embed_size,weights=[distilbert_embeddings])(encoder_inp)\n    print(encoder_inp.shape)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    #Decoder Block\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(distilbert_embeddings.shape[0],embed_size,weights=[distilbert_embeddings])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs,64)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(5,activation='softmax')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n\n\n\n#Use the method for creating distilbert embeddings\n\ndistilbert_embeddings = fetch_vectors(train_df.question_body.values,'distilbert-base-uncased')\nmax_features=distilbert_embeddings.shape[0]\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\nmodel=distilbert_encoder_decoder_attention(maxlen,max_features,distilbert_embeddings)  \nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"distilbert_encoder_decoder_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=10,verbose=2)\n","execution_count":10,"outputs":[{"output_type":"stream","text":"Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_projector', 'vocab_transform', 'vocab_layer_norm']\n- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\nToken indices sequence length is longer than the specified maximum sequence length for this model (1414 > 512). Running this sequence through the model will result in indexing errors\n","name":"stderr"},{"output_type":"stream","text":"Padded and Tokenized Training Sequence (4863, 500)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 500)\nTarget Values Shape (1216,)\n(None, 500)\nEncoder Ouputs Shape(None, 60)\nModel: \"functional_5\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_5 (InputLayer)            [(None, 500)]        0                                            \n__________________________________________________________________________________________________\nembedding_4 (Embedding)         (None, 500, 768)     4668672     input_5[0][0]                    \n__________________________________________________________________________________________________\ninput_6 (InputLayer)            [(None, 500)]        0                                            \n__________________________________________________________________________________________________\nlstm_4 (LSTM)                   [(None, 60), (None,  198960      embedding_4[0][0]                \n__________________________________________________________________________________________________\nembedding_5 (Embedding)         (None, 500, 768)     4668672     input_6[0][0]                    \n__________________________________________________________________________________________________\nscaled__dot__product__self__att ((None,), (None, 60) 7320        lstm_4[0][1]                     \n                                                                 lstm_4[0][1]                     \n                                                                 lstm_4[0][0]                     \n                                                                 lstm_4[0][2]                     \n                                                                 lstm_4[0][2]                     \n                                                                 lstm_4[0][0]                     \n__________________________________________________________________________________________________\nlstm_5 (LSTM)                   [(None, 60), (None,  198960      embedding_5[0][0]                \n                                                                 scaled__dot__product__self__atten\n                                                                 scaled__dot__product__self__atten\n__________________________________________________________________________________________________\ndense_13 (Dense)                (None, 16)           976         lstm_5[0][0]                     \n__________________________________________________________________________________________________\ndense_14 (Dense)                (None, 5)            85          dense_13[0][0]                   \n==================================================================================================\nTotal params: 9,743,645\nTrainable params: 9,743,645\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/10\n10/10 - 6s - loss: 1.5006 - accuracy: 0.3991\nEpoch 2/10\n10/10 - 6s - loss: 1.4683 - accuracy: 0.4004\nEpoch 3/10\n10/10 - 6s - loss: 1.4363 - accuracy: 0.4004\nEpoch 4/10\n10/10 - 7s - loss: 1.3748 - accuracy: 0.4010\nEpoch 5/10\n10/10 - 6s - loss: 1.2808 - accuracy: 0.4429\nEpoch 6/10\n10/10 - 6s - loss: 1.1316 - accuracy: 0.5116\nEpoch 7/10\n10/10 - 6s - loss: 1.0156 - accuracy: 0.5924\nEpoch 8/10\n10/10 - 6s - loss: 0.8204 - accuracy: 0.6837\nEpoch 9/10\n10/10 - 6s - loss: 0.8423 - accuracy: 0.6952\nEpoch 10/10\n10/10 - 6s - loss: 0.6608 - accuracy: 0.7905\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f4f003771d0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture of 1 block Transformer with Distilbert Embeddings\n\nThe model architecture can be shown as below:\n\n<img src=\"https://i.imgur.com/m9tgoG6.png\">"},{"metadata":{},"cell_type":"markdown","source":"## Testing With Albert Embeddings\n\n[Albert](https://arxiv.org/abs/1909.11942) is a lightweight bert which introduces parameter sharing, caching, and intermediate repeated splitting of the embedding matrix for efficient modelling tasks.\n\nAccording to the paper:\n\n'The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT.'\n\n\n<img src=\"data:image/jpeg;base64,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\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Final 1 Block Transformer which consists of 1 encoder decoder with albert embedding\nimport math\nmaxlen=500\nembed_size=768\n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    #A class for Self Attention- Q,K,V dimensions\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n        \ndef fetch_vectors(string_list,pretrained_model,batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFAlbertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        #This is the attention mask\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n        #Extract the last hidden states.\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\n\n\ndef albert_encoder_decoder_attention(maxlen,max_features,albert_embeddings):\n    #Creating LSTM  encoder neural model with albert pretrained embeddings\n    #Encoder Block\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(encoder_inp)\n    print(encoder_inp.shape)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    #Decoder Block\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs,64)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(5,activation='softmax')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n\n\n\n#Use the method for creating albert embeddings\n\nalbert_embeddings = fetch_vectors(train_df.question_body.values,'albert-base-v1')\nmax_features=albert_embeddings.shape[0]\nmodel=albert_encoder_decoder_attention(maxlen,max_features,albert_embeddings)  \nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"albert_encoder_decoder_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['question_body'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=train_y\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=10,verbose=2)\n","execution_count":12,"outputs":[{"output_type":"stream","text":"Some layers from the model checkpoint at albert-base-v1 were not used when initializing TFAlbertModel: ['predictions']\n- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFAlbertModel were initialized from the model checkpoint at albert-base-v1.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\nToken indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n","name":"stderr"},{"output_type":"stream","text":"(None, 500)\nEncoder Ouputs Shape(None, 60)\nModel: \"functional_9\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_9 (InputLayer)            [(None, 500)]        0                                            \n__________________________________________________________________________________________________\nembedding_8 (Embedding)         (None, 500, 768)     4668672     input_9[0][0]                    \n__________________________________________________________________________________________________\ninput_10 (InputLayer)           [(None, 500)]        0                                            \n__________________________________________________________________________________________________\nlstm_8 (LSTM)                   [(None, 60), (None,  198960      embedding_8[0][0]                \n__________________________________________________________________________________________________\nembedding_9 (Embedding)         (None, 500, 768)     4668672     input_10[0][0]                   \n__________________________________________________________________________________________________\nscaled__dot__product__self__att ((None,), (None, 60) 7320        lstm_8[0][1]                     \n                                                                 lstm_8[0][1]                     \n                                                                 lstm_8[0][0]                     \n                                                                 lstm_8[0][2]                     \n                                                                 lstm_8[0][2]                     \n                                                                 lstm_8[0][0]                     \n__________________________________________________________________________________________________\nlstm_9 (LSTM)                   [(None, 60), (None,  198960      embedding_9[0][0]                \n                                                                 scaled__dot__product__self__atten\n                                                                 scaled__dot__product__self__atten\n__________________________________________________________________________________________________\ndense_23 (Dense)                (None, 16)           976         lstm_9[0][0]                     \n__________________________________________________________________________________________________\ndense_24 (Dense)                (None, 5)            85          dense_23[0][0]                   \n==================================================================================================\nTotal params: 9,743,645\nTrainable params: 9,743,645\nNon-trainable params: 0\n__________________________________________________________________________________________________\nPadded and Tokenized Training Sequence (4863, 500)\nTarget Values Shape (4863,)\nPadded and Tokenized Training Sequence (1216, 500)\nTarget Values Shape (1216,)\nEpoch 1/10\n10/10 - 6s - loss: 1.5943 - accuracy: 0.2770\nEpoch 2/10\n10/10 - 6s - loss: 1.5051 - accuracy: 0.4004\nEpoch 3/10\n10/10 - 6s - loss: 1.4926 - accuracy: 0.4004\nEpoch 4/10\n10/10 - 6s - loss: 1.4802 - accuracy: 0.4004\nEpoch 5/10\n10/10 - 6s - loss: 1.4655 - accuracy: 0.4004\nEpoch 6/10\n10/10 - 6s - loss: 1.4412 - accuracy: 0.4004\nEpoch 7/10\n10/10 - 6s - loss: 1.3964 - accuracy: 0.4051\nEpoch 8/10\n10/10 - 6s - loss: 1.3407 - accuracy: 0.4370\nEpoch 9/10\n10/10 - 6s - loss: 1.2704 - accuracy: 0.4884\nEpoch 10/10\n10/10 - 6s - loss: 1.2057 - accuracy: 0.5139\n","name":"stdout"},{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f4f3c6173d0>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture on Albert\n\n\nThe model architecture can be shown as follows:\n\n\n<img src=\"https://i.imgur.com/ELj9zmF.png\">"},{"metadata":{},"cell_type":"markdown","source":"# Modifying the Codebase to Create a 2 Block Transformer\n\nWe will now be modifying the codebase and create a 2 block transformer architecture. Traditionally  a classical transformer is made of 8 blocks, and BERT variants have 12/24 such blocks. The most important aspect of these blocks is that only intermediate hidden cell states (h) get communicated . So between each encoder block we can keep an attention block to modify the internal h states of the LSTM stacks . Similarly between each decoder we can create a transmission of the hidden h states of the decoder cells.\n\nThis looks like the image shown below:\n\n<img src=\"http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png\">"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    #A class for Self Attention- Q,K,V dimensions\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n\n\n\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n        \ndef fetch_vectors(string_list,pretrained_model,batch_size=64):\n    # inspired by https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFAlbertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        #This is the attention mask\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n        #Extract the last hidden states.\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\n\n\n\ndef albert_encoder_decoder_attention(maxlen,max_features,albert_embeddings):\n    #Creating LSTM  encoder neural model with distilbert pretrained embeddings\n    #Encoder Block-I\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(encoder_inp)\n    print(encoder_inp.shape)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Encoder Block -II\n#     encoder_inp_2=Input(shape=(maxlen,))\n#     encoder_embed_2=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(encoder_inp_2)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    \n    encoder_embed_attention_h,encoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs,64)\n    encoder_embed_attention_c,encoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs,64)\n    encoder_lstm_cell_2=LSTM(60,return_sequences='True',return_state=True)\n    encoder_outputs_2,encoder_state_lstm_h,encoder_state_lstm_c=encoder_lstm_cell_2(encoder_embed,initial_state=[encoder_embed_wghts_h,encoder_embed_wghts_c])\n    print(f'Second Encoder Ouputs Shape{encoder_outputs.shape}')\n    \n    \n    #Decoder Block - I\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs_2,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs_2,64)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n    \n    #Decoder Block - II\n#     decoder_inp=Input(shape=(maxlen,))\n#     decoder_embed=Embedding(albert_embeddings.shape[0],embed_size,weights=[albert_embeddings])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(decoder_state_lstm_h,decoder_state_lstm_h,decoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(decoder_state_lstm_c,decoder_state_lstm_c,decoder_outputs,64)\n    decoder_lstm_cell_2=LSTM(60,return_state=True)\n    decoder_outputs_2,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell_2(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n    \n    #Final FFNN - Dense Layer\n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs_2)\n    decoder_dense_cell2=Dense(5,activation='softmax')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n\n\nmaxlen=500\nembed_size=768\n#Use the method for creating  embeddings\nalbert_embeddings = fetch_vectors(train_df.question_body.values,'albert-base-v1')\n\nmodel=albert_encoder_decoder_attention(maxlen,max_features,albert_embeddings)  \nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"albert_encoder_decoder_attention_II.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=10,verbose=2)\n","execution_count":15,"outputs":[{"output_type":"stream","text":"Some layers from the model checkpoint at albert-base-v1 were not used when initializing TFAlbertModel: ['predictions']\n- This IS expected if you are initializing TFAlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFAlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFAlbertModel were initialized from the model checkpoint at albert-base-v1.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFAlbertModel for predictions without further training.\nToken indices sequence length is longer than the specified maximum sequence length for this model (557 > 512). Running this sequence through the model will result in indexing errors\n","name":"stderr"},{"output_type":"stream","text":"(None, 500)\nEncoder Ouputs Shape(None, 60)\nSecond Encoder Ouputs Shape(None, 60)\nModel: \"functional_12\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_19 (InputLayer)           [(None, 500)]        0                                            \n__________________________________________________________________________________________________\nembedding_18 (Embedding)        (None, 500, 768)     4668672     input_19[0][0]                   \n__________________________________________________________________________________________________\nlstm_17 (LSTM)                  [(None, 60), (None,  198960      embedding_18[0][0]               \n__________________________________________________________________________________________________\nscaled__dot__product__self__att ((None,), (None, 60) 7320        lstm_17[0][1]                    \n                                                                 lstm_17[0][1]                    \n                                                                 lstm_17[0][0]                    \n                                                                 lstm_17[0][2]                    \n                                                                 lstm_17[0][2]                    \n                                                                 lstm_17[0][0]                    \n__________________________________________________________________________________________________\ninput_20 (InputLayer)           [(None, 500)]        0                                            \n__________________________________________________________________________________________________\nlstm_18 (LSTM)                  [(None, 500, 60), (N 198960      embedding_18[0][0]               \n                                                                 scaled__dot__product__self__atten\n                                                                 scaled__dot__product__self__atten\n__________________________________________________________________________________________________\nembedding_19 (Embedding)        (None, 500, 768)     4668672     input_20[0][0]                   \n__________________________________________________________________________________________________\nscaled__dot__product__self__att ((None, 60), (None,  7320        lstm_17[0][1]                    \n                                                                 lstm_17[0][1]                    \n                                                                 lstm_18[0][0]                    \n                                                                 lstm_17[0][2]                    \n                                                                 lstm_17[0][2]                    \n                                                                 lstm_18[0][0]                    \n__________________________________________________________________________________________________\nlstm_19 (LSTM)                  [(None, 500, 60), (N 198960      embedding_19[0][0]               \n                                                                 scaled__dot__product__self__atten\n                                                                 scaled__dot__product__self__atten\n__________________________________________________________________________________________________\nscaled__dot__product__self__att ((None, 60), (None,  7320        lstm_19[0][1]                    \n                                                                 lstm_19[0][1]                    \n                                                                 lstm_19[0][0]                    \n                                                                 lstm_19[0][2]                    \n                                                                 lstm_19[0][2]                    \n                                                                 lstm_19[0][0]                    \n__________________________________________________________________________________________________\nlstm_20 (LSTM)                  [(None, 60), (None,  198960      embedding_19[0][0]               \n                                                                 scaled__dot__product__self__atten\n                                                                 scaled__dot__product__self__atten\n__________________________________________________________________________________________________\ndense_54 (Dense)                (None, 16)           976         lstm_20[0][0]                    \n__________________________________________________________________________________________________\ndense_55 (Dense)                (None, 5)            85          dense_54[0][0]                   \n==================================================================================================\nTotal params: 10,156,205\nTrainable params: 10,156,205\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/10\n10/10 - 7s - loss: 1.5827 - accuracy: 0.2054\nEpoch 2/10\n10/10 - 7s - loss: 1.4995 - accuracy: 0.3708\nEpoch 3/10\n10/10 - 7s - loss: 1.4729 - accuracy: 0.4004\nEpoch 4/10\n10/10 - 7s - loss: 1.4529 - accuracy: 0.4004\nEpoch 5/10\n10/10 - 7s - loss: 1.4141 - accuracy: 0.4004\nEpoch 6/10\n10/10 - 7s - loss: 1.3455 - accuracy: 0.4045\nEpoch 7/10\n10/10 - 7s - loss: 1.2682 - accuracy: 0.4261\nEpoch 8/10\n10/10 - 7s - loss: 1.1552 - accuracy: 0.4526\nEpoch 9/10\n10/10 - 7s - loss: 1.0561 - accuracy: 0.5166\nEpoch 10/10\n10/10 - 7s - loss: 0.9483 - accuracy: 0.5844\n","name":"stdout"},{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f4b41755d10>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Model Architecture of 2 Block Transformer with Albert Embeddings\n\n\nThe model architecture can be defined as follows:\n\n\n<img src=\"https://i.imgur.com/6Jn2xz7.png\">"},{"metadata":{},"cell_type":"markdown","source":"# Conclusion of Transformers \n\nIn this case, we saw and built a 2Block transformer with intermediate Albert Embeddings (quite complex) along with Self Attention multiple times (Multi head self attention). And there are many other ways to build a classifier, and [this Kernel](https://www.kaggle.com/abhilash1910/nlp-workshop-2-ml-india) provides a good overview of creating another high performant Classifier using TPU Cluster (Google Cloud Storage) in Kaggle.With the help of Huggingface we can create any language model for any NLP tasks. Now we will be looking into a classic example of creating answers from a given context with a QA Transformer model."},{"metadata":{},"cell_type":"markdown","source":"# Question Answering with Transformers Pipeline\n\n\n\n<img src=\"https://huggingface.co/front/assets/huggingface_logo.svg\">\n\n\n[HuggingFace Pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) are extensively used in any language modelling tasks from NER, to QA, MNLI  or even Summarisation. The pipeline is built to be robust and very efficient to handle the different downstream tasks from a transformer model. Once the embeddings are received from a Transformer model, the downstream tasks can be solved very simply by taking those tokenized embeddings and solving it for our downstream tasks. Particularly in the context of question answering this is done by using some segment tokens (tagging questions with tag 1 and answers with tag 0). Here we will be looking how to build a simple pipeline to extract answers from a given context when questions are provided.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering,pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"abhilash1910/distilbert-squadv1\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"abhilash1910/distilbert-squadv1\")\ncontext=train_df['answer'][2]\nprint('The context:',context)\nquestion='What is the thickness of the boards?'\nqa_pipeline=pipeline('question-answering',model=model,tokenizer=tokenizer)\nqa_inputs={\n    'question':question,\n    'context':context\n}\nfinal_container=qa_pipeline(qa_inputs)\nprint('The final answer: ',final_container['answer'])\nprint('The corresponding score: ',final_container['score'])\n","execution_count":24,"outputs":[{"output_type":"stream","text":"The context: Do you even need grooves?  We make several products using through-hole components that are intended to mount using VHB double-sided foam tape.  The boards are 0.062\" thick double-sided with PTH and we use a table-top vertical belt sander to bring the component leads almost flush with the solder mask.  In other words, the solder mask isn't touched by the sand paper but the leads are all sanded flat and sitting just proud of the solder mask.\n\nThis works well for small boards.\n\nFor what it's worth, there are commercial machines available that use a rotary saw blade to do the same thing.  The board is held horizontal in a mounting / clamping system on the base and the saw motor is vertical on a sliding X-Y mechanism.  The saw blade simply cuts all of the leads almost flush with the board surface.  \n\nThis system is suited for boards of all sizes but especially for those boards larger than can be handled easily to be sanded with the belt sander.\n\nAlso note that these techniques are suitable only for PC boards with plated-through holes.  \n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n  FutureWarning,\n","name":"stderr"},{"output_type":"stream","text":"The final answer:  0.062\"\nThe corresponding score:  0.21890029311180115\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## A Detailed Example From Huggingface\n\nThis provides a detailed overview of using [Huggingface Library](https://huggingface.co/transformers/usage.html) for creating a QA model. Here the following steps are to be followed:\n\n\n- Tokenize the context and questions.Special Tokens can be added as well\n- After the tokenization is completed, we have to convert the encoded tokens to ids\n- Pass these ids (segment ids) into the Transformer model to extract the last embedding output\n- Collect the start and end scores from the model output\n- Understand the corresponding ids which are present in those ranges\n- Convert those ids back into tokens (words)\n- These converted tokens gives the answer\n\nSome important points:\n\n- We only consider ourselves with the segment tokens. In the case of BERT, there are 3 such tokens- inputs, position and segments \n- Masking of tokens is optional and can be used\n- For QA models BERT variants perform really well as opposed to GPT variants."},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\nimport tensorflow as tf\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nmodel = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n\ntext = r\"\"\"\n🤗 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\narchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural\nLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\nTensorFlow 2.0 and PyTorch.\n\"\"\"\n\nquestions = [\n    \"How many pretrained models are available in Transformers?\",\n    \"What does Transformers provide?\",\n    \"Transformers provides interoperability between which frameworks?\",\n]\n\nfor question in questions:\n    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"tf\")\n    input_ids = inputs[\"input_ids\"].numpy()[0]\n\n    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    answer_start_scores, answer_end_scores = model(inputs)\n\n    answer_start = tf.argmax(\n        answer_start_scores, axis=1\n    ).numpy()[0]  # Get the most likely beginning of answer with the argmax of the score\n    answer_end = (\n        tf.argmax(answer_end_scores, axis=1) + 1\n    ).numpy()[0]  # Get the most likely end of answer with the argmax of the score\n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\\n\")","execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=443.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd85ba961af419e9e3313001c207b49"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afae1ab83c364ae5871e2970936e05fe"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1341090760.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f97489e6243543e9bee885d7e44d0dab"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n\nAll the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n","name":"stderr"},{"output_type":"stream","text":"Question: How many pretrained models are available in Transformers?\nAnswer: over 32 +\n\nQuestion: What does Transformers provide?\nAnswer: general - purpose architectures\n\nQuestion: Transformers provides interoperability between which frameworks?\nAnswer: tensorflow 2 . 0 and pytorch\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Creating an NER pipeline with Transformers\n\nUsing the huggingface pipeline we can create finetune our models for downstream tasks as well. We use the same code example as in the question answer model (using pipeline).There are many ways to do this:\n\n- We can use the pipeline ('ner') as it is\n- We can also use any models , and for that we can specify model name and tokenizer name from huggingface (same as in the QA example)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#Without using any specific model as such\nner_pipeline=pipeline('ner')\ncontext=train_df['answer'][100]\nprint('Context',context)\nprint('NER tagging of the sample')\nprint(ner_pipeline(context))","execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=998.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53a1cdb74f6a4883bc20b43785cf3c1f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1334448817.0, style=ProgressStyle(descr…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6007d7dcdff4d52a3a5e6c7540a315a"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abdc86806f4a45c6af9e3ff4447a1b45"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=60.0, style=ProgressStyle(description_w…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9239a322954b4cfc9339fd408877687f"}},"metadata":{}},{"output_type":"stream","text":"\nContext The National Express website is presumably the one you mean, where it shows it off the African coast.\n\nHowever, if you look further down the page, it says:\n\n\n  London (Shoreditch)\n  \n  Bethnal Green Rd (to Stansted Airport: Stop J\n  \n  opp Overground Stn; or\n  \n  from Stansted: Stop K)\n\n\nBethnal Green road on Google Maps clearly shows the road  running west-east, with the Overground station indicated on the same map.\n\nHope that helps!\n\nNER tagging of the sample\n[{'word': 'National', 'score': 0.9990436434745789, 'entity': 'I-ORG', 'index': 2}, {'word': 'Express', 'score': 0.9986949563026428, 'entity': 'I-ORG', 'index': 3}, {'word': 'African', 'score': 0.9867666959762573, 'entity': 'I-MISC', 'index': 18}, {'word': 'London', 'score': 0.9930039048194885, 'entity': 'I-LOC', 'index': 34}, {'word': 'Shore', 'score': 0.9949707984924316, 'entity': 'I-LOC', 'index': 36}, {'word': '##dit', 'score': 0.9466100335121155, 'entity': 'I-LOC', 'index': 37}, {'word': '##ch', 'score': 0.9857004880905151, 'entity': 'I-LOC', 'index': 38}, {'word': 'Beth', 'score': 0.994072675704956, 'entity': 'I-LOC', 'index': 40}, {'word': '##nal', 'score': 0.9927289485931396, 'entity': 'I-LOC', 'index': 41}, {'word': 'Green', 'score': 0.9962114691734314, 'entity': 'I-LOC', 'index': 42}, {'word': 'Rd', 'score': 0.8821483254432678, 'entity': 'I-LOC', 'index': 43}, {'word': 'Stan', 'score': 0.9967549443244934, 'entity': 'I-LOC', 'index': 46}, {'word': '##sted', 'score': 0.9917051792144775, 'entity': 'I-LOC', 'index': 47}, {'word': 'Airport', 'score': 0.9757152199745178, 'entity': 'I-LOC', 'index': 48}, {'word': 'Over', 'score': 0.7517663836479187, 'entity': 'I-ORG', 'index': 54}, {'word': '##ground', 'score': 0.688988983631134, 'entity': 'I-ORG', 'index': 55}, {'word': 'St', 'score': 0.3473934829235077, 'entity': 'I-ORG', 'index': 56}, {'word': 'Stan', 'score': 0.9980659484863281, 'entity': 'I-LOC', 'index': 61}, {'word': '##sted', 'score': 0.9959512948989868, 'entity': 'I-LOC', 'index': 62}, {'word': 'Beth', 'score': 0.9920918345451355, 'entity': 'I-LOC', 'index': 67}, {'word': '##nal', 'score': 0.991579532623291, 'entity': 'I-LOC', 'index': 68}, {'word': 'Green', 'score': 0.9975249767303467, 'entity': 'I-LOC', 'index': 69}, {'word': 'Google', 'score': 0.9408113360404968, 'entity': 'I-MISC', 'index': 72}, {'word': 'Map', 'score': 0.8091458678245544, 'entity': 'I-MISC', 'index': 73}, {'word': '##s', 'score': 0.5909797549247742, 'entity': 'I-MISC', 'index': 74}, {'word': 'Over', 'score': 0.947101354598999, 'entity': 'I-ORG', 'index': 86}, {'word': '##ground', 'score': 0.9270796775817871, 'entity': 'I-ORG', 'index': 87}]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nexample = \"My name is Wolfgang and I live in Berlin\"\n\nner_results = nlp(example)\nprint(ner_results)","execution_count":1,"outputs":[{"output_type":"stream","text":"[{'word': 'Wolfgang', 'score': 0.9990139603614807, 'entity': 'B-PER', 'index': 4}, {'word': 'Berlin', 'score': 0.9996449947357178, 'entity': 'B-LOC', 'index': 9}]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Conclusion of Session\n\nWe have come to the end of the session, abd we have explored many things from the basics of neural networks to creating an advanced architecture . Also we have explored the different world of embeddings,created our SOTA Transformer models with embeddings. We also saw how to create a 2 block Transformer, and saw the different downstream tasks like QA modelling and NER where Transformers can be used so easily. There are lots of concepts in Transformers which have not yet been covered but this provides a gentle introduction how to scale a simple neural network to a Transformer.\n\n\n<img src=\"https://media.tenor.com/images/cfbe42db018b64c9806a6b4ae89f3f2c/tenor.gif\">"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}